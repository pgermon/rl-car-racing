{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "dqn_agent.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnQxMTDetqk1",
        "outputId": "dd72cd64-2f7a-40b8-da65-6d1584ccd39d"
      },
      "source": [
        "!pip install keras-rl2\n",
        "!pip install 'gym[all]'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras-rl2 in /usr/local/lib/python3.6/dist-packages (1.0.4)\n",
            "Requirement already satisfied: tensorflow>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from keras-rl2) (2.4.1)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->keras-rl2) (3.12.4)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->keras-rl2) (1.12)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->keras-rl2) (1.1.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->keras-rl2) (1.19.5)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->keras-rl2) (2.4.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->keras-rl2) (1.6.3)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->keras-rl2) (0.10.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->keras-rl2) (3.7.4.3)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->keras-rl2) (3.3.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->keras-rl2) (1.12.1)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->keras-rl2) (2.4.1)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->keras-rl2) (1.15.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->keras-rl2) (0.36.2)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->keras-rl2) (1.1.2)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->keras-rl2) (2.10.0)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->keras-rl2) (1.32.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->keras-rl2) (0.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tensorflow>=2.1.0->keras-rl2) (51.3.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (0.4.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (1.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (3.3.3)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (1.17.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (3.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (4.2.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (2020.12.5)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (3.4.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (0.4.8)\n",
            "Requirement already satisfied: gym[all] in /usr/local/lib/python3.6/dist-packages (0.17.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym[all]) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[all]) (1.19.5)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[all]) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[all]) (1.5.0)\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"all\" in /usr/local/lib/python3.6/dist-packages (from gym[all]) (0.2.6)\n",
            "Collecting mujoco-py<2.0,>=1.50; extra == \"all\"\n",
            "  Using cached https://files.pythonhosted.org/packages/cf/8c/64e0630b3d450244feef0688d90eab2448631e40ba6bdbd90a70b84898e7/mujoco-py-1.50.1.68.tar.gz\n",
            "Requirement already satisfied: opencv-python; extra == \"all\" in /usr/local/lib/python3.6/dist-packages (from gym[all]) (4.1.2.30)\n",
            "Requirement already satisfied: Pillow; extra == \"all\" in /usr/local/lib/python3.6/dist-packages (from gym[all]) (7.0.0)\n",
            "Requirement already satisfied: box2d-py~=2.3.5; extra == \"all\" in /usr/local/lib/python3.6/dist-packages (from gym[all]) (2.3.8)\n",
            "Requirement already satisfied: imageio; extra == \"all\" in /usr/local/lib/python3.6/dist-packages (from gym[all]) (2.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[all]) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from atari-py~=0.2.0; extra == \"all\"->gym[all]) (1.15.0)\n",
            "Requirement already satisfied: glfw>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from mujoco-py<2.0,>=1.50; extra == \"all\"->gym[all]) (2.0.0)\n",
            "Requirement already satisfied: Cython>=0.27.2 in /usr/local/lib/python3.6/dist-packages (from mujoco-py<2.0,>=1.50; extra == \"all\"->gym[all]) (0.29.21)\n",
            "Requirement already satisfied: cffi>=1.10 in /usr/local/lib/python3.6/dist-packages (from mujoco-py<2.0,>=1.50; extra == \"all\"->gym[all]) (1.14.4)\n",
            "Requirement already satisfied: lockfile>=0.12.2 in /usr/local/lib/python3.6/dist-packages (from mujoco-py<2.0,>=1.50; extra == \"all\"->gym[all]) (0.12.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.10->mujoco-py<2.0,>=1.50; extra == \"all\"->gym[all]) (2.20)\n",
            "Building wheels for collected packages: mujoco-py\n",
            "  Building wheel for mujoco-py (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for mujoco-py\u001b[0m\n",
            "\u001b[?25h  Running setup.py clean for mujoco-py\n",
            "Failed to build mujoco-py\n",
            "Installing collected packages: mujoco-py\n",
            "    Running setup.py install for mujoco-py ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31mERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-psyzy_hd/mujoco-py/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-psyzy_hd/mujoco-py/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-9pb2d6xg/install-record.txt --single-version-externally-managed --compile Check the logs for full command output.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kiJAUeZR5CT",
        "outputId": "cfc9dd73-9d97-4794-92b8-8f24b5ed5e83"
      },
      "source": [
        "%%bash\n",
        "\n",
        "# install required system dependencies\n",
        "sudo apt-get install -y xvfb x11-utils\n",
        "\n",
        "# install required python dependencies (might need to install additional gym extras depending)\n",
        "pip install gym[box2d]==0.17.* pyvirtualdisplay==0.2.* PyOpenGL==3.1.* PyOpenGL-accelerate==3.1.*"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "x11-utils is already the newest version (7.7+3build1).\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.8).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 13 not upgraded.\n",
            "Requirement already satisfied: gym[box2d]==0.17.* in /usr/local/lib/python3.6/dist-packages (0.17.3)\n",
            "Requirement already satisfied: pyvirtualdisplay==0.2.* in /usr/local/lib/python3.6/dist-packages (0.2.5)\n",
            "Requirement already satisfied: PyOpenGL==3.1.* in /usr/local/lib/python3.6/dist-packages (3.1.5)\n",
            "Requirement already satisfied: PyOpenGL-accelerate==3.1.* in /usr/local/lib/python3.6/dist-packages (3.1.5)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[box2d]==0.17.*) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym[box2d]==0.17.*) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[box2d]==0.17.*) (1.19.5)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[box2d]==0.17.*) (1.5.0)\n",
            "Requirement already satisfied: box2d-py~=2.3.5; extra == \"box2d\" in /usr/local/lib/python3.6/dist-packages (from gym[box2d]==0.17.*) (2.3.8)\n",
            "Requirement already satisfied: EasyProcess in /usr/local/lib/python3.6/dist-packages (from pyvirtualdisplay==0.2.*) (0.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[box2d]==0.17.*) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QcJFd7SR6dA"
      },
      "source": [
        "import pyvirtualdisplay\n",
        "\n",
        "\n",
        "_display = pyvirtualdisplay.Display(visible=False,  # use False with Xvfb\n",
        "                                    size=(1400, 900))\n",
        "_ = _display.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9umkJgH0tXxJ"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import gym\n",
        "from gym.envs.registration import registry, register, make, spec\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Flatten, Convolution2D, Permute, Input, Reshape\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.core import Processor\n",
        "from rl.callbacks import FileLogger, ModelIntervalCheckpoint\n",
        "\n",
        "from car_racing_v1 import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROyM0qPHtXxT"
      },
      "source": [
        "register(\n",
        "    id='CarRacing-v1',\n",
        "    entry_point='gym.envs.box2d:CarRacing',\n",
        "    max_episode_steps=2000,\n",
        "    reward_threshold=900,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WSeAnmktXxV",
        "outputId": "f71b0a9b-e888-4ca1-d993-38f48a4e8e01"
      },
      "source": [
        "# Get the environment and extract the number of actions.\n",
        "ENV_NAME = 'CarRacing-v1'\n",
        "env = CarRacing()\n",
        "\n",
        "nb_actions = len(env.action_space)\n",
        "input_shape = env.observation_space.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgUsfNcttXxX",
        "outputId": "c032dfb8-ef4a-4fa4-f8b7-d78cf46fb9ee"
      },
      "source": [
        "print(\"nb actions = \", len(env.action_space))\n",
        "print(\"observation_space.shape = \", input_shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nb actions =  16\n",
            "observation_space.shape =  (96, 96, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZuuGYBCotXxa"
      },
      "source": [
        "def build_model(input_shape, nb_actions):\n",
        "    model = Sequential()\n",
        "    print(input_shape)\n",
        "    \n",
        "    model.add(Reshape(input_shape, input_shape = (1,96,96,3)))\n",
        "    model.add(Convolution2D(32, (8, 8)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Convolution2D(64, (4, 4)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Convolution2D(64, (3, 3)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dense(nb_actions))\n",
        "    model.add(Activation('linear'))\n",
        "    \n",
        "    \n",
        "    '''model.add(Flatten())\n",
        "    model.add(Dense(24, activation='relu'))\n",
        "    model.add(Dense(24, activation='relu'))\n",
        "    model.add(Dense(nb_actions, activation='linear'))'''\n",
        "    \n",
        "    print(model.summary())\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jb4miTk6tXxb",
        "outputId": "dccde3f3-bd8d-4c42-bf3d-2ec511764d38"
      },
      "source": [
        "model = build_model(input_shape, nb_actions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(96, 96, 3)\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape (Reshape)            (None, 96, 96, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 89, 89, 32)        6176      \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 89, 89, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 86, 86, 64)        32832     \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 86, 86, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 84, 84, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 84, 84, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 451584)            0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 512)               231211520 \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 16)                8208      \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 16)                0         \n",
            "=================================================================\n",
            "Total params: 231,295,664\n",
            "Trainable params: 231,295,664\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5yn_wNbtXxd"
      },
      "source": [
        "def build_agent(model, nb_actions):\n",
        "    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.05, nb_steps=10000)\n",
        "    memory = SequentialMemory(limit=50000, window_length=1)\n",
        "    dqn = DQNAgent(model=model, memory=memory, policy=policy, \n",
        "                  nb_actions=nb_actions, nb_steps_warmup=500, target_model_update=100)\n",
        "    return dqn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQQLONQVtXxd"
      },
      "source": [
        "dqn = build_agent(model, nb_actions)\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZ9kIWNftXxf"
      },
      "source": [
        "weights_filename = 'dqn_' + ENV_NAME + '_weights.h5f'\n",
        "checkpoint_weights_filename = 'dqn_' + ENV_NAME + '_weights_{step}.h5f'\n",
        "log_filename = 'dqn_' + ENV_NAME + '_log.json'\n",
        "callbacks = [ModelIntervalCheckpoint(checkpoint_weights_filename, interval=2500)]\n",
        "callbacks += [FileLogger(log_filename, interval=100)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQ-ADkhCeVUe",
        "outputId": "54b58941-835c-4153-fabe-9aeed1821cb0"
      },
      "source": [
        "import os\r\n",
        "if os.path.exists(weights_filename + '.data-00000-of-00001') and os.path.exists(weights_filename + '.index'):\r\n",
        "  dqn.load_weights(weights_filename)\r\n",
        "  print('Weights loaded')\r\n",
        "else:\r\n",
        "  print('Weights not found')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Weights loaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NiTQOrCmtXxf",
        "outputId": "5e59baaf-778f-42a0-a306-95e364602337"
      },
      "source": [
        "dqn.fit(env, nb_steps=3000, verbose=2, nb_max_episode_steps=200, action_repetition=3, visualize = False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for 3000 steps ...\n",
            "Track generation: 1187..1488 -> 301-tiles track\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "   18/3000: episode: 1, duration: 8.003s, episode steps:  18, steps per second:   2, episode reward: -95.200, mean reward: -5.289 [-100.100,  6.367], mean action: 6.944 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1107..1388 -> 281-tiles track\n",
            "   36/3000: episode: 2, duration: 0.506s, episode steps:  18, steps per second:  36, episode reward: -94.486, mean reward: -5.249 [-100.100,  6.843], mean action: 8.556 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1156..1455 -> 299-tiles track\n",
            "   54/3000: episode: 3, duration: 0.483s, episode steps:  18, steps per second:  37, episode reward: -95.133, mean reward: -5.285 [-100.100,  6.411], mean action: 7.556 [1.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1160..1455 -> 295-tiles track\n",
            "   72/3000: episode: 4, duration: 0.529s, episode steps:  18, steps per second:  34, episode reward: -94.996, mean reward: -5.278 [-100.100,  6.503], mean action: 9.222 [3.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1231..1543 -> 312-tiles track\n",
            "   90/3000: episode: 5, duration: 0.491s, episode steps:  18, steps per second:  37, episode reward: -95.554, mean reward: -5.309 [-100.100,  6.131], mean action: 6.833 [2.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1139..1428 -> 289-tiles track\n",
            "  108/3000: episode: 6, duration: 0.777s, episode steps:  18, steps per second:  23, episode reward: -94.783, mean reward: -5.266 [-100.100,  6.644], mean action: 7.222 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1156..1449 -> 293-tiles track\n",
            "  126/3000: episode: 7, duration: 0.568s, episode steps:  18, steps per second:  32, episode reward: -91.601, mean reward: -5.089 [-100.200,  6.549], mean action: 6.889 [1.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1172..1469 -> 297-tiles track\n",
            "  144/3000: episode: 8, duration: 0.491s, episode steps:  18, steps per second:  37, episode reward: -91.786, mean reward: -5.099 [-100.200,  6.457], mean action: 6.500 [2.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1177..1475 -> 298-tiles track\n",
            "  162/3000: episode: 9, duration: 0.483s, episode steps:  18, steps per second:  37, episode reward: -95.099, mean reward: -5.283 [-100.100,  6.434], mean action: 8.778 [1.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1277..1600 -> 323-tiles track\n",
            "  180/3000: episode: 10, duration: 0.512s, episode steps:  18, steps per second:  35, episode reward: -95.883, mean reward: -5.327 [-100.100,  5.911], mean action: 6.278 [1.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1116..1405 -> 289-tiles track\n",
            "  198/3000: episode: 11, duration: 0.483s, episode steps:  18, steps per second:  37, episode reward: -94.783, mean reward: -5.266 [-100.100,  6.644], mean action: 7.667 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1047..1313 -> 266-tiles track\n",
            "  215/3000: episode: 12, duration: 0.754s, episode steps:  17, steps per second:  23, episode reward: -97.553, mean reward: -5.738 [-100.300,  7.247], mean action: 8.588 [1.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1087..1363 -> 276-tiles track\n",
            "  233/3000: episode: 13, duration: 0.492s, episode steps:  18, steps per second:  37, episode reward: -94.291, mean reward: -5.238 [-100.100,  6.973], mean action: 8.333 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1204..1509 -> 305-tiles track\n",
            "  251/3000: episode: 14, duration: 0.631s, episode steps:  18, steps per second:  29, episode reward: -95.332, mean reward: -5.296 [-100.100,  6.279], mean action: 7.778 [1.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1076..1356 -> 280-tiles track\n",
            "  269/3000: episode: 15, duration: 0.483s, episode steps:  18, steps per second:  37, episode reward: -94.447, mean reward: -5.247 [-100.100,  6.868], mean action: 7.167 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1208..1514 -> 306-tiles track\n",
            "  287/3000: episode: 16, duration: 0.509s, episode steps:  18, steps per second:  35, episode reward: -92.185, mean reward: -5.121 [-100.200,  6.257], mean action: 7.500 [2.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1182..1481 -> 299-tiles track\n",
            "  305/3000: episode: 17, duration: 0.782s, episode steps:  18, steps per second:  23, episode reward: -95.133, mean reward: -5.285 [-100.100,  6.411], mean action: 6.889 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 928..1166 -> 238-tiles track\n",
            "retry to generate track (normal if there are not manyinstances of this message)\n",
            "Track generation: 962..1206 -> 244-tiles track\n",
            "  323/3000: episode: 18, duration: 0.478s, episode steps:  18, steps per second:  38, episode reward: -92.854, mean reward: -5.159 [-100.100,  7.930], mean action: 9.111 [3.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1217..1525 -> 308-tiles track\n",
            "  341/3000: episode: 19, duration: 0.489s, episode steps:  18, steps per second:  37, episode reward: -95.428, mean reward: -5.302 [-100.100,  6.215], mean action: 7.667 [1.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1149..1440 -> 291-tiles track\n",
            "  359/3000: episode: 20, duration: 0.493s, episode steps:  18, steps per second:  37, episode reward: -94.855, mean reward: -5.270 [-100.100,  6.597], mean action: 7.333 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1007..1263 -> 256-tiles track\n",
            "  377/3000: episode: 21, duration: 0.483s, episode steps:  18, steps per second:  37, episode reward: -93.435, mean reward: -5.191 [-100.100,  7.543], mean action: 6.278 [1.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1395..1748 -> 353-tiles track\n",
            "  395/3000: episode: 22, duration: 0.507s, episode steps:  18, steps per second:  36, episode reward: -96.677, mean reward: -5.371 [-100.100,  5.382], mean action: 6.611 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 941..1182 -> 241-tiles track\n",
            "retry to generate track (normal if there are not manyinstances of this message)\n",
            "Track generation: 1160..1454 -> 294-tiles track\n",
            "  413/3000: episode: 23, duration: 0.779s, episode steps:  18, steps per second:  23, episode reward: -94.961, mean reward: -5.276 [-100.100,  6.526], mean action: 8.000 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1159..1453 -> 294-tiles track\n",
            "  431/3000: episode: 24, duration: 0.486s, episode steps:  18, steps per second:  37, episode reward: -94.961, mean reward: -5.276 [-100.100,  6.526], mean action: 7.000 [1.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1122..1414 -> 292-tiles track\n",
            "  449/3000: episode: 25, duration: 0.475s, episode steps:  18, steps per second:  38, episode reward: -91.554, mean reward: -5.086 [-100.200,  6.573], mean action: 7.000 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1055..1323 -> 268-tiles track\n",
            "  467/3000: episode: 26, duration: 0.767s, episode steps:  18, steps per second:  23, episode reward: -90.319, mean reward: -5.018 [-100.200,  7.191], mean action: 7.611 [1.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1029..1296 -> 267-tiles track\n",
            "  485/3000: episode: 27, duration: 0.481s, episode steps:  18, steps per second:  37, episode reward: -93.922, mean reward: -5.218 [-100.100,  7.219], mean action: 8.333 [1.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1208..1519 -> 311-tiles track\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  503/3000: episode: 28, duration: 6.847s, episode steps:  18, steps per second:   3, episode reward: -95.523, mean reward: -5.307 [-100.100,  6.152], mean action: 5.944 [0.000, 15.000],  loss: 71.524910, mae: 44.471125, mean_q: -43.849440, mean_eps: 0.954865\n",
            "Track generation: 1131..1418 -> 287-tiles track\n",
            "  521/3000: episode: 29, duration: 5.076s, episode steps:  18, steps per second:   4, episode reward: -94.710, mean reward: -5.262 [-100.100,  6.693], mean action: 5.278 [0.000, 15.000],  loss: 120.545693, mae: 44.106391, mean_q: -43.596812, mean_eps: 0.953965\n",
            "Track generation: 1059..1328 -> 269-tiles track\n",
            "  539/3000: episode: 30, duration: 5.085s, episode steps:  18, steps per second:   4, episode reward: -90.375, mean reward: -5.021 [-100.200,  7.163], mean action: 5.556 [0.000, 14.000],  loss: 76.274684, mae: 43.859257, mean_q: -42.906503, mean_eps: 0.952345\n",
            "Track generation: 1134..1426 -> 292-tiles track\n",
            "retry to generate track (normal if there are not manyinstances of this message)\n",
            "Track generation: 1154..1447 -> 293-tiles track\n",
            "  557/3000: episode: 31, duration: 5.169s, episode steps:  18, steps per second:   3, episode reward: -94.926, mean reward: -5.274 [-100.100,  6.549], mean action: 7.333 [0.000, 15.000],  loss: 91.761369, mae: 43.791972, mean_q: -43.155669, mean_eps: 0.950725\n",
            "Track generation: 1166..1461 -> 295-tiles track\n",
            "  575/3000: episode: 32, duration: 5.106s, episode steps:  18, steps per second:   4, episode reward: -94.996, mean reward: -5.278 [-100.100,  6.503], mean action: 7.333 [0.000, 13.000],  loss: 87.628078, mae: 43.900597, mean_q: -42.869534, mean_eps: 0.949105\n",
            "Track generation: 1086..1370 -> 284-tiles track\n",
            "  593/3000: episode: 33, duration: 4.999s, episode steps:  18, steps per second:   4, episode reward: -94.599, mean reward: -5.256 [-100.100,  6.767], mean action: 7.167 [1.000, 15.000],  loss: 91.752365, mae: 44.071935, mean_q: -43.394156, mean_eps: 0.947485\n",
            "Track generation: 1199..1503 -> 304-tiles track\n",
            "  611/3000: episode: 34, duration: 5.387s, episode steps:  18, steps per second:   3, episode reward: -95.299, mean reward: -5.294 [-100.100,  6.301], mean action: 7.389 [1.000, 15.000],  loss: 75.247717, mae: 43.841135, mean_q: -42.656918, mean_eps: 0.945865\n",
            "Track generation: 1364..1709 -> 345-tiles track\n",
            "  630/3000: episode: 35, duration: 5.420s, episode steps:  19, steps per second:   4, episode reward: -85.251, mean reward: -4.487 [-100.200,  5.514], mean action: 4.053 [0.000, 12.000],  loss: 74.002468, mae: 42.661880, mean_q: -41.901494, mean_eps: 0.944200\n",
            "Track generation: 924..1159 -> 235-tiles track\n",
            "  648/3000: episode: 36, duration: 5.116s, episode steps:  18, steps per second:   4, episode reward: -92.379, mean reward: -5.132 [-100.100,  8.247], mean action: 8.000 [0.000, 14.000],  loss: 96.183137, mae: 42.218868, mean_q: -41.948830, mean_eps: 0.942535\n",
            "Track generation: 1254..1577 -> 323-tiles track\n",
            "  666/3000: episode: 37, duration: 5.174s, episode steps:  18, steps per second:   3, episode reward: -92.878, mean reward: -5.160 [-100.200,  5.911], mean action: 6.444 [0.000, 15.000],  loss: 95.573377, mae: 42.566008, mean_q: -42.519499, mean_eps: 0.940915\n",
            "Track generation: 1008..1264 -> 256-tiles track\n",
            "  684/3000: episode: 38, duration: 5.169s, episode steps:  18, steps per second:   3, episode reward: -89.614, mean reward: -4.979 [-100.200,  7.543], mean action: 6.389 [0.000, 15.000],  loss: 114.048882, mae: 42.996517, mean_q: -42.056794, mean_eps: 0.939295\n",
            "Track generation: 1241..1560 -> 319-tiles track\n",
            "  702/3000: episode: 39, duration: 5.476s, episode steps:  18, steps per second:   3, episode reward: -95.766, mean reward: -5.320 [-100.100,  5.989], mean action: 7.222 [0.000, 15.000],  loss: 80.808866, mae: 43.364111, mean_q: -42.331236, mean_eps: 0.937675\n",
            "Track generation: 1225..1534 -> 309-tiles track\n",
            "  720/3000: episode: 40, duration: 5.176s, episode steps:  18, steps per second:   3, episode reward: -95.460, mean reward: -5.303 [-100.100,  6.194], mean action: 7.167 [0.000, 15.000],  loss: 77.448169, mae: 42.578171, mean_q: -41.421140, mean_eps: 0.936055\n",
            "Track generation: 1173..1470 -> 297-tiles track\n",
            "  738/3000: episode: 41, duration: 5.090s, episode steps:  18, steps per second:   4, episode reward: -95.065, mean reward: -5.281 [-100.100,  6.457], mean action: 6.611 [2.000, 15.000],  loss: 93.651362, mae: 41.738456, mean_q: -41.320317, mean_eps: 0.934435\n",
            "Track generation: 1183..1483 -> 300-tiles track\n",
            "  756/3000: episode: 42, duration: 5.017s, episode steps:  18, steps per second:   4, episode reward: -95.167, mean reward: -5.287 [-100.100,  6.389], mean action: 7.167 [1.000, 14.000],  loss: 107.708095, mae: 42.188829, mean_q: -41.908515, mean_eps: 0.932815\n",
            "Track generation: 1120..1403 -> 283-tiles track\n",
            "  774/3000: episode: 43, duration: 5.122s, episode steps:  18, steps per second:   4, episode reward: -94.562, mean reward: -5.253 [-100.100,  6.792], mean action: 7.111 [1.000, 15.000],  loss: 77.922217, mae: 42.612462, mean_q: -41.201437, mean_eps: 0.931195\n",
            "Track generation: 1240..1554 -> 314-tiles track\n",
            "  792/3000: episode: 44, duration: 5.114s, episode steps:  18, steps per second:   4, episode reward: -89.426, mean reward: -4.968 [-100.300,  6.090], mean action: 5.278 [0.000, 12.000],  loss: 63.547626, mae: 42.080917, mean_q: -40.884876, mean_eps: 0.929575\n",
            "Track generation: 1202..1507 -> 305-tiles track\n",
            "  810/3000: episode: 45, duration: 5.401s, episode steps:  18, steps per second:   3, episode reward: -95.332, mean reward: -5.296 [-100.100,  6.279], mean action: 7.944 [0.000, 15.000],  loss: 120.613256, mae: 41.864975, mean_q: -40.980266, mean_eps: 0.927955\n",
            "Track generation: 1207..1513 -> 306-tiles track\n",
            "  828/3000: episode: 46, duration: 5.138s, episode steps:  18, steps per second:   4, episode reward: -95.364, mean reward: -5.298 [-100.100,  6.257], mean action: 8.222 [1.000, 15.000],  loss: 82.855505, mae: 41.845343, mean_q: -40.346626, mean_eps: 0.926335\n",
            "Track generation: 1155..1448 -> 293-tiles track\n",
            "  846/3000: episode: 47, duration: 5.123s, episode steps:  18, steps per second:   4, episode reward: -94.926, mean reward: -5.274 [-100.100,  6.549], mean action: 9.778 [0.000, 15.000],  loss: 95.254311, mae: 41.565257, mean_q: -40.570192, mean_eps: 0.924715\n",
            "Track generation: 1144..1434 -> 290-tiles track\n",
            "  864/3000: episode: 48, duration: 5.141s, episode steps:  18, steps per second:   4, episode reward: -94.819, mean reward: -5.268 [-100.100,  6.620], mean action: 6.722 [0.000, 15.000],  loss: 78.435987, mae: 41.257116, mean_q: -40.466281, mean_eps: 0.923095\n",
            "Track generation: 1228..1539 -> 311-tiles track\n",
            "  882/3000: episode: 49, duration: 5.109s, episode steps:  18, steps per second:   4, episode reward: -95.523, mean reward: -5.307 [-100.100,  6.152], mean action: 8.167 [3.000, 15.000],  loss: 102.013401, mae: 41.094272, mean_q: -40.040327, mean_eps: 0.921475\n",
            "Track generation: 1170..1472 -> 302-tiles track\n",
            "  900/3000: episode: 50, duration: 5.421s, episode steps:  18, steps per second:   3, episode reward: -95.233, mean reward: -5.291 [-100.100,  6.345], mean action: 6.944 [0.000, 15.000],  loss: 80.853746, mae: 41.290114, mean_q: -40.321004, mean_eps: 0.919855\n",
            "Track generation: 1188..1489 -> 301-tiles track\n",
            "  918/3000: episode: 51, duration: 5.401s, episode steps:  18, steps per second:   3, episode reward: -95.200, mean reward: -5.289 [-100.100,  6.367], mean action: 7.611 [1.000, 15.000],  loss: 66.534469, mae: 41.003221, mean_q: -39.893092, mean_eps: 0.918235\n",
            "Track generation: 1206..1512 -> 306-tiles track\n",
            "  935/3000: episode: 52, duration: 5.177s, episode steps:  17, steps per second:   3, episode reward: -98.543, mean reward: -5.797 [-100.300,  6.257], mean action: 10.412 [3.000, 15.000],  loss: 75.167151, mae: 39.906845, mean_q: -38.757423, mean_eps: 0.916660\n",
            "Track generation: 1160..1460 -> 300-tiles track\n",
            "  953/3000: episode: 53, duration: 5.115s, episode steps:  18, steps per second:   4, episode reward: -95.167, mean reward: -5.287 [-100.100,  6.389], mean action: 7.944 [0.000, 15.000],  loss: 91.618780, mae: 39.869951, mean_q: -39.102047, mean_eps: 0.915085\n",
            "Track generation: 1154..1447 -> 293-tiles track\n",
            "  971/3000: episode: 54, duration: 5.043s, episode steps:  18, steps per second:   4, episode reward: -94.926, mean reward: -5.274 [-100.100,  6.549], mean action: 8.722 [0.000, 15.000],  loss: 109.828809, mae: 40.316979, mean_q: -40.042513, mean_eps: 0.913465\n",
            "Track generation: 1086..1367 -> 281-tiles track\n",
            "retry to generate track (normal if there are not manyinstances of this message)\n",
            "Track generation: 1082..1363 -> 281-tiles track\n",
            "  989/3000: episode: 55, duration: 5.091s, episode steps:  18, steps per second:   4, episode reward: -87.543, mean reward: -4.863 [-100.300,  6.843], mean action: 5.278 [1.000, 13.000],  loss: 105.332352, mae: 41.088274, mean_q: -40.550315, mean_eps: 0.911845\n",
            "Track generation: 1171..1468 -> 297-tiles track\n",
            " 1007/3000: episode: 56, duration: 5.382s, episode steps:  18, steps per second:   3, episode reward: -88.508, mean reward: -4.917 [-100.300,  6.457], mean action: 5.278 [0.000, 15.000],  loss: 114.078298, mae: 40.975746, mean_q: -39.226264, mean_eps: 0.910225\n",
            "Track generation: 1243..1558 -> 315-tiles track\n",
            " 1025/3000: episode: 57, duration: 5.079s, episode steps:  18, steps per second:   4, episode reward: -95.646, mean reward: -5.314 [-100.100,  6.069], mean action: 6.778 [0.000, 15.000],  loss: 108.832827, mae: 40.710819, mean_q: -38.828463, mean_eps: 0.908605\n",
            "Track generation: 1148..1439 -> 291-tiles track\n",
            " 1043/3000: episode: 58, duration: 5.119s, episode steps:  18, steps per second:   4, episode reward: -91.507, mean reward: -5.084 [-100.200,  6.597], mean action: 5.944 [1.000, 15.000],  loss: 96.329368, mae: 39.916330, mean_q: -38.348310, mean_eps: 0.906985\n",
            "Track generation: 1231..1550 -> 319-tiles track\n",
            " 1061/3000: episode: 59, duration: 5.102s, episode steps:  18, steps per second:   4, episode reward: -95.766, mean reward: -5.320 [-100.100,  5.989], mean action: 8.944 [1.000, 15.000],  loss: 106.354935, mae: 39.721937, mean_q: -38.321505, mean_eps: 0.905365\n",
            "Track generation: 1202..1517 -> 315-tiles track\n",
            " 1079/3000: episode: 60, duration: 5.090s, episode steps:  18, steps per second:   4, episode reward: -95.646, mean reward: -5.314 [-100.100,  6.069], mean action: 8.444 [1.000, 14.000],  loss: 99.758212, mae: 39.788899, mean_q: -38.300940, mean_eps: 0.903745\n",
            "Track generation: 1196..1499 -> 303-tiles track\n",
            " 1097/3000: episode: 61, duration: 5.160s, episode steps:  18, steps per second:   3, episode reward: -95.266, mean reward: -5.293 [-100.100,  6.323], mean action: 9.722 [1.000, 15.000],  loss: 94.566528, mae: 39.657914, mean_q: -38.034472, mean_eps: 0.902125\n",
            "Track generation: 1029..1290 -> 261-tiles track\n",
            " 1115/3000: episode: 62, duration: 5.370s, episode steps:  18, steps per second:   3, episode reward: -93.662, mean reward: -5.203 [-100.100,  7.392], mean action: 7.778 [0.000, 13.000],  loss: 116.835437, mae: 39.298999, mean_q: -38.175996, mean_eps: 0.900505\n",
            "Track generation: 1067..1348 -> 281-tiles track\n",
            " 1133/3000: episode: 63, duration: 5.110s, episode steps:  18, steps per second:   4, episode reward: -91.014, mean reward: -5.056 [-96.629,  6.843], mean action: 7.167 [1.000, 14.000],  loss: 131.444493, mae: 39.296009, mean_q: -38.080664, mean_eps: 0.898885\n",
            "Track generation: 1125..1411 -> 286-tiles track\n",
            " 1151/3000: episode: 64, duration: 5.117s, episode steps:  18, steps per second:   4, episode reward: -91.265, mean reward: -5.070 [-100.200,  6.718], mean action: 5.500 [0.000, 15.000],  loss: 114.730543, mae: 39.527665, mean_q: -37.665298, mean_eps: 0.897265\n",
            "Track generation: 1316..1649 -> 333-tiles track\n",
            " 1169/3000: episode: 65, duration: 5.124s, episode steps:  18, steps per second:   4, episode reward: -96.164, mean reward: -5.342 [-100.100,  5.724], mean action: 7.667 [0.000, 14.000],  loss: 66.793870, mae: 38.829679, mean_q: -37.097612, mean_eps: 0.895645\n",
            "Track generation: 1016..1282 -> 266-tiles track\n",
            " 1187/3000: episode: 66, duration: 5.114s, episode steps:  18, steps per second:   4, episode reward: -93.879, mean reward: -5.216 [-100.100,  7.247], mean action: 8.500 [1.000, 13.000],  loss: 101.137476, mae: 38.077510, mean_q: -37.326075, mean_eps: 0.894025\n",
            "Track generation: 1332..1669 -> 337-tiles track\n",
            " 1205/3000: episode: 67, duration: 5.463s, episode steps:  18, steps per second:   3, episode reward: -96.271, mean reward: -5.348 [-100.100,  5.652], mean action: 7.778 [0.000, 15.000],  loss: 95.188125, mae: 37.973403, mean_q: -37.859747, mean_eps: 0.892405\n",
            "Track generation: 1155..1448 -> 293-tiles track\n",
            " 1223/3000: episode: 68, duration: 5.149s, episode steps:  18, steps per second:   3, episode reward: -94.926, mean reward: -5.274 [-100.100,  6.549], mean action: 8.278 [1.000, 13.000],  loss: 116.754194, mae: 38.300122, mean_q: -38.058952, mean_eps: 0.890785\n",
            "Track generation: 1079..1357 -> 278-tiles track\n",
            " 1241/3000: episode: 69, duration: 5.171s, episode steps:  18, steps per second:   3, episode reward: -94.370, mean reward: -5.243 [-100.100,  6.920], mean action: 8.889 [0.000, 15.000],  loss: 115.748918, mae: 38.610533, mean_q: -37.732951, mean_eps: 0.889165\n",
            "Track generation: 1030..1295 -> 265-tiles track\n",
            "retry to generate track (normal if there are not manyinstances of this message)\n",
            "Track generation: 1057..1325 -> 268-tiles track\n",
            " 1259/3000: episode: 70, duration: 5.226s, episode steps:  18, steps per second:   3, episode reward: -93.964, mean reward: -5.220 [-100.100,  7.191], mean action: 7.111 [0.000, 15.000],  loss: 129.024801, mae: 38.841672, mean_q: -37.710357, mean_eps: 0.887545\n",
            "Track generation: 1153..1445 -> 292-tiles track\n",
            " 1277/3000: episode: 71, duration: 5.166s, episode steps:  18, steps per second:   3, episode reward: -94.891, mean reward: -5.272 [-100.100,  6.573], mean action: 7.278 [1.000, 15.000],  loss: 85.678843, mae: 38.856920, mean_q: -37.482167, mean_eps: 0.885925\n",
            "Track generation: 1100..1386 -> 286-tiles track\n",
            " 1295/3000: episode: 72, duration: 5.104s, episode steps:  18, steps per second:   4, episode reward: -94.674, mean reward: -5.260 [-100.100,  6.718], mean action: 7.111 [1.000, 15.000],  loss: 132.368921, mae: 38.657880, mean_q: -37.434365, mean_eps: 0.884305\n",
            "Track generation: 1263..1583 -> 320-tiles track\n",
            " 1313/3000: episode: 73, duration: 5.397s, episode steps:  18, steps per second:   3, episode reward: -95.796, mean reward: -5.322 [-100.100,  5.970], mean action: 6.000 [0.000, 13.000],  loss: 110.155058, mae: 38.913365, mean_q: -38.209151, mean_eps: 0.882685\n",
            "Track generation: 1049..1323 -> 274-tiles track\n",
            " 1331/3000: episode: 74, duration: 5.115s, episode steps:  18, steps per second:   4, episode reward: -94.211, mean reward: -5.234 [-100.100,  7.026], mean action: 6.722 [0.000, 15.000],  loss: 104.061759, mae: 38.937161, mean_q: -37.896015, mean_eps: 0.881065\n",
            "Track generation: 1304..1634 -> 330-tiles track\n",
            " 1349/3000: episode: 75, duration: 5.145s, episode steps:  18, steps per second:   3, episode reward: -90.202, mean reward: -5.011 [-100.300,  5.779], mean action: 5.111 [0.000, 15.000],  loss: 112.666502, mae: 38.944381, mean_q: -38.709799, mean_eps: 0.879445\n",
            "Track generation: 1085..1362 -> 277-tiles track\n",
            " 1367/3000: episode: 76, duration: 5.118s, episode steps:  18, steps per second:   4, episode reward: -94.330, mean reward: -5.241 [-100.100,  6.946], mean action: 8.167 [0.000, 15.000],  loss: 97.945126, mae: 38.899396, mean_q: -38.540678, mean_eps: 0.877825\n",
            "Track generation: 904..1141 -> 237-tiles track\n",
            " 1385/3000: episode: 77, duration: 5.124s, episode steps:  18, steps per second:   4, episode reward: -92.488, mean reward: -5.138 [-100.100,  8.175], mean action: 9.167 [0.000, 15.000],  loss: 111.977452, mae: 38.804383, mean_q: -38.649817, mean_eps: 0.876205\n",
            "Track generation: 1071..1343 -> 272-tiles track\n",
            " 1403/3000: episode: 78, duration: 5.430s, episode steps:  18, steps per second:   3, episode reward: -94.130, mean reward: -5.229 [-100.100,  7.080], mean action: 8.000 [0.000, 15.000],  loss: 89.369922, mae: 38.928659, mean_q: -38.886661, mean_eps: 0.874585\n",
            "Track generation: 1210..1527 -> 317-tiles track\n",
            " 1421/3000: episode: 79, duration: 5.117s, episode steps:  18, steps per second:   4, episode reward: -95.706, mean reward: -5.317 [-100.100,  6.029], mean action: 8.389 [0.000, 15.000],  loss: 87.561045, mae: 38.654206, mean_q: -38.257010, mean_eps: 0.872965\n",
            "Track generation: 1153..1445 -> 292-tiles track\n",
            " 1439/3000: episode: 80, duration: 5.133s, episode steps:  18, steps per second:   4, episode reward: -94.891, mean reward: -5.272 [-100.100,  6.573], mean action: 8.111 [0.000, 15.000],  loss: 93.685861, mae: 38.776013, mean_q: -38.685968, mean_eps: 0.871345\n",
            "Track generation: 1276..1599 -> 323-tiles track\n",
            " 1457/3000: episode: 81, duration: 5.098s, episode steps:  18, steps per second:   4, episode reward: -95.883, mean reward: -5.327 [-100.100,  5.911], mean action: 8.222 [1.000, 15.000],  loss: 98.996586, mae: 39.011984, mean_q: -38.569061, mean_eps: 0.869725\n",
            "Track generation: 1141..1431 -> 290-tiles track\n",
            " 1475/3000: episode: 82, duration: 5.089s, episode steps:  18, steps per second:   4, episode reward: -94.819, mean reward: -5.268 [-100.100,  6.620], mean action: 8.556 [0.000, 15.000],  loss: 108.503638, mae: 38.884212, mean_q: -38.094787, mean_eps: 0.868105\n",
            "Track generation: 1185..1485 -> 300-tiles track\n",
            " 1492/3000: episode: 83, duration: 4.839s, episode steps:  17, steps per second:   4, episode reward: -98.411, mean reward: -5.789 [-100.300,  6.389], mean action: 9.235 [1.000, 15.000],  loss: 102.691321, mae: 39.285307, mean_q: -38.253779, mean_eps: 0.866530\n",
            "Track generation: 1280..1604 -> 324-tiles track\n",
            " 1510/3000: episode: 84, duration: 5.406s, episode steps:  18, steps per second:   3, episode reward: -92.916, mean reward: -5.162 [-100.200,  5.892], mean action: 6.000 [0.000, 14.000],  loss: 79.033090, mae: 39.002614, mean_q: -38.691439, mean_eps: 0.864955\n",
            "Track generation: 1150..1442 -> 292-tiles track\n",
            " 1528/3000: episode: 85, duration: 5.091s, episode steps:  18, steps per second:   4, episode reward: -94.891, mean reward: -5.272 [-100.100,  6.573], mean action: 7.000 [0.000, 13.000],  loss: 118.714681, mae: 38.978611, mean_q: -39.571959, mean_eps: 0.863335\n",
            "Track generation: 1140..1429 -> 289-tiles track\n",
            " 1545/3000: episode: 86, duration: 4.834s, episode steps:  17, steps per second:   4, episode reward: -98.156, mean reward: -5.774 [-100.300,  6.644], mean action: 9.294 [1.000, 15.000],  loss: 122.253128, mae: 39.501793, mean_q: -40.089496, mean_eps: 0.861760\n",
            "Track generation: 1170..1468 -> 298-tiles track\n",
            "retry to generate track (normal if there are not manyinstances of this message)\n",
            "Track generation: 1207..1513 -> 306-tiles track\n",
            " 1563/3000: episode: 87, duration: 5.093s, episode steps:  18, steps per second:   4, episode reward: -95.364, mean reward: -5.298 [-100.100,  6.257], mean action: 7.278 [0.000, 15.000],  loss: 101.860712, mae: 39.784833, mean_q: -39.990378, mean_eps: 0.860185\n",
            "Track generation: 1079..1361 -> 282-tiles track\n",
            " 1581/3000: episode: 88, duration: 5.115s, episode steps:  18, steps per second:   4, episode reward: -91.065, mean reward: -5.059 [-100.200,  6.817], mean action: 5.667 [0.000, 15.000],  loss: 115.449274, mae: 39.503710, mean_q: -38.878725, mean_eps: 0.858565\n",
            "Track generation: 1196..1499 -> 303-tiles track\n",
            " 1599/3000: episode: 89, duration: 5.103s, episode steps:  18, steps per second:   4, episode reward: -95.266, mean reward: -5.293 [-100.100,  6.323], mean action: 7.667 [0.000, 14.000],  loss: 108.610678, mae: 39.556613, mean_q: -38.743856, mean_eps: 0.856945\n",
            "Track generation: 1233..1545 -> 312-tiles track\n",
            " 1617/3000: episode: 90, duration: 5.353s, episode steps:  18, steps per second:   3, episode reward: -95.554, mean reward: -5.309 [-100.100,  6.131], mean action: 7.111 [0.000, 15.000],  loss: 85.077881, mae: 39.541802, mean_q: -38.854013, mean_eps: 0.855325\n",
            "Track generation: 1116..1399 -> 283-tiles track\n",
            " 1635/3000: episode: 91, duration: 5.110s, episode steps:  18, steps per second:   4, episode reward: -94.562, mean reward: -5.253 [-100.100,  6.792], mean action: 7.278 [1.000, 15.000],  loss: 101.455162, mae: 39.350822, mean_q: -38.844209, mean_eps: 0.853705\n",
            "Track generation: 1147..1438 -> 291-tiles track\n",
            " 1652/3000: episode: 92, duration: 4.774s, episode steps:  17, steps per second:   4, episode reward: -98.203, mean reward: -5.777 [-100.300,  6.597], mean action: 9.529 [1.000, 15.000],  loss: 97.853874, mae: 38.986783, mean_q: -38.732942, mean_eps: 0.852130\n",
            "Track generation: 1026..1292 -> 266-tiles track\n",
            " 1670/3000: episode: 93, duration: 4.952s, episode steps:  18, steps per second:   4, episode reward: -93.879, mean reward: -5.216 [-100.100,  7.247], mean action: 7.556 [2.000, 15.000],  loss: 95.167184, mae: 39.042016, mean_q: -38.545735, mean_eps: 0.850555\n",
            "Track generation: 1287..1613 -> 326-tiles track\n",
            " 1687/3000: episode: 94, duration: 4.751s, episode steps:  17, steps per second:   4, episode reward: -98.946, mean reward: -5.820 [-100.300,  5.854], mean action: 9.176 [1.000, 15.000],  loss: 82.769816, mae: 39.061431, mean_q: -38.721048, mean_eps: 0.848980\n",
            "Track generation: 1088..1364 -> 276-tiles track\n",
            " 1704/3000: episode: 95, duration: 5.014s, episode steps:  17, steps per second:   3, episode reward: -97.827, mean reward: -5.755 [-100.300,  6.973], mean action: 8.765 [0.000, 15.000],  loss: 93.225273, mae: 39.061350, mean_q: -39.136939, mean_eps: 0.847450\n",
            "Track generation: 1119..1411 -> 292-tiles track\n",
            " 1721/3000: episode: 96, duration: 4.822s, episode steps:  17, steps per second:   4, episode reward: -98.227, mean reward: -5.778 [-100.300,  6.573], mean action: 8.000 [2.000, 13.000],  loss: 112.410838, mae: 39.096555, mean_q: -38.902084, mean_eps: 0.845920\n",
            "Track generation: 1240..1554 -> 314-tiles track\n",
            " 1738/3000: episode: 97, duration: 4.807s, episode steps:  17, steps per second:   4, episode reward: -98.710, mean reward: -5.806 [-100.300,  6.090], mean action: 9.353 [0.000, 15.000],  loss: 77.133388, mae: 39.457547, mean_q: -39.066380, mean_eps: 0.844390\n",
            "Track generation: 1031..1293 -> 262-tiles track\n",
            " 1756/3000: episode: 98, duration: 5.117s, episode steps:  18, steps per second:   4, episode reward: -93.706, mean reward: -5.206 [-100.100,  7.363], mean action: 5.889 [0.000, 13.000],  loss: 96.016259, mae: 39.323886, mean_q: -38.711225, mean_eps: 0.842815\n",
            "Track generation: 1132..1419 -> 287-tiles track\n",
            " 1773/3000: episode: 99, duration: 4.813s, episode steps:  17, steps per second:   4, episode reward: -98.107, mean reward: -5.771 [-100.300,  6.693], mean action: 9.824 [0.000, 15.000],  loss: 107.409445, mae: 39.204709, mean_q: -39.045695, mean_eps: 0.841240\n",
            "Track generation: 1223..1533 -> 310-tiles track\n",
            " 1791/3000: episode: 100, duration: 5.112s, episode steps:  18, steps per second:   4, episode reward: -95.491, mean reward: -5.305 [-100.100,  6.172], mean action: 5.667 [0.000, 15.000],  loss: 91.053742, mae: 39.626912, mean_q: -39.432605, mean_eps: 0.839665\n",
            "Track generation: 1275..1598 -> 323-tiles track\n",
            " 1809/3000: episode: 101, duration: 5.378s, episode steps:  18, steps per second:   3, episode reward: -95.883, mean reward: -5.327 [-100.100,  5.911], mean action: 8.778 [1.000, 15.000],  loss: 100.635002, mae: 39.573768, mean_q: -39.208753, mean_eps: 0.838045\n",
            "Track generation: 1215..1523 -> 308-tiles track\n",
            " 1827/3000: episode: 102, duration: 5.066s, episode steps:  18, steps per second:   4, episode reward: -92.271, mean reward: -5.126 [-100.200,  6.215], mean action: 6.444 [0.000, 15.000],  loss: 112.369939, mae: 39.498952, mean_q: -38.826320, mean_eps: 0.836425\n",
            "Track generation: 1241..1564 -> 323-tiles track\n",
            " 1845/3000: episode: 103, duration: 5.092s, episode steps:  18, steps per second:   4, episode reward: -95.883, mean reward: -5.327 [-100.100,  5.911], mean action: 7.056 [0.000, 15.000],  loss: 98.422314, mae: 39.474176, mean_q: -39.250699, mean_eps: 0.834805\n",
            "Track generation: 1222..1535 -> 313-tiles track\n",
            "retry to generate track (normal if there are not manyinstances of this message)\n",
            "Track generation: 1077..1357 -> 280-tiles track\n",
            " 1863/3000: episode: 104, duration: 6.035s, episode steps:  18, steps per second:   3, episode reward: -94.447, mean reward: -5.247 [-100.100,  6.868], mean action: 9.111 [0.000, 15.000],  loss: 97.573687, mae: 39.448566, mean_q: -39.282340, mean_eps: 0.833185\n",
            "Track generation: 1138..1427 -> 289-tiles track\n",
            " 1881/3000: episode: 105, duration: 5.102s, episode steps:  18, steps per second:   4, episode reward: -94.783, mean reward: -5.266 [-100.100,  6.644], mean action: 8.000 [3.000, 15.000],  loss: 105.514936, mae: 39.483424, mean_q: -39.555185, mean_eps: 0.831565\n",
            "Track generation: 1135..1430 -> 295-tiles track\n",
            " 1899/3000: episode: 106, duration: 5.142s, episode steps:  18, steps per second:   4, episode reward: -91.695, mean reward: -5.094 [-100.200,  6.503], mean action: 6.278 [0.000, 15.000],  loss: 98.488083, mae: 39.606891, mean_q: -39.234539, mean_eps: 0.829945\n",
            "Track generation: 1234..1547 -> 313-tiles track\n",
            " 1917/3000: episode: 107, duration: 5.346s, episode steps:  18, steps per second:   3, episode reward: -95.585, mean reward: -5.310 [-100.100,  6.110], mean action: 9.167 [2.000, 15.000],  loss: 82.140780, mae: 39.467355, mean_q: -39.193690, mean_eps: 0.828325\n",
            "Track generation: 1155..1448 -> 293-tiles track\n",
            " 1935/3000: episode: 108, duration: 5.100s, episode steps:  18, steps per second:   4, episode reward: -94.926, mean reward: -5.274 [-100.100,  6.549], mean action: 8.556 [2.000, 15.000],  loss: 93.542125, mae: 39.092684, mean_q: -39.469581, mean_eps: 0.826705\n",
            "Track generation: 1068..1339 -> 271-tiles track\n",
            " 1953/3000: episode: 109, duration: 5.105s, episode steps:  18, steps per second:   4, episode reward: -94.089, mean reward: -5.227 [-100.100,  7.107], mean action: 7.389 [1.000, 13.000],  loss: 97.810328, mae: 39.290197, mean_q: -39.754178, mean_eps: 0.825085\n",
            "Track generation: 1049..1319 -> 270-tiles track\n",
            "retry to generate track (normal if there are not manyinstances of this message)\n",
            "Track generation: 1193..1495 -> 302-tiles track\n",
            " 1971/3000: episode: 110, duration: 5.081s, episode steps:  18, steps per second:   4, episode reward: -95.233, mean reward: -5.291 [-100.100,  6.345], mean action: 7.056 [0.000, 14.000],  loss: 103.035650, mae: 39.636217, mean_q: -40.136866, mean_eps: 0.823465\n",
            "Track generation: 1032..1295 -> 263-tiles track\n",
            " 1988/3000: episode: 111, duration: 4.809s, episode steps:  17, steps per second:   4, episode reward: -97.466, mean reward: -5.733 [-100.300,  7.334], mean action: 10.588 [2.000, 15.000],  loss: 81.874550, mae: 39.789422, mean_q: -40.284958, mean_eps: 0.821890\n",
            "Track generation: 1192..1494 -> 302-tiles track\n",
            " 2006/3000: episode: 112, duration: 5.377s, episode steps:  18, steps per second:   3, episode reward: -95.233, mean reward: -5.291 [-100.100,  6.345], mean action: 6.667 [1.000, 12.000],  loss: 95.392982, mae: 39.245028, mean_q: -39.303604, mean_eps: 0.820315\n",
            "Track generation: 1103..1383 -> 280-tiles track\n",
            " 2024/3000: episode: 113, duration: 5.089s, episode steps:  18, steps per second:   4, episode reward: -94.447, mean reward: -5.247 [-100.100,  6.868], mean action: 7.556 [0.000, 15.000],  loss: 108.918291, mae: 39.158092, mean_q: -39.330023, mean_eps: 0.818695\n",
            "Track generation: 1176..1474 -> 298-tiles track\n",
            " 2042/3000: episode: 114, duration: 5.096s, episode steps:  18, steps per second:   4, episode reward: -88.565, mean reward: -4.920 [-100.300,  6.434], mean action: 5.000 [0.000, 12.000],  loss: 93.986139, mae: 39.477597, mean_q: -39.741920, mean_eps: 0.817075\n",
            "Track generation: 1160..1454 -> 294-tiles track\n",
            " 2060/3000: episode: 115, duration: 5.047s, episode steps:  18, steps per second:   4, episode reward: -94.961, mean reward: -5.276 [-100.100,  6.526], mean action: 7.333 [0.000, 14.000],  loss: 95.393404, mae: 39.496872, mean_q: -39.915110, mean_eps: 0.815455\n",
            "Track generation: 949..1199 -> 250-tiles track\n",
            " 2077/3000: episode: 116, duration: 4.830s, episode steps:  17, steps per second:   4, episode reward: -97.068, mean reward: -5.710 [-100.300,  7.732], mean action: 7.647 [1.000, 12.000],  loss: 97.017561, mae: 39.234080, mean_q: -39.494061, mean_eps: 0.813880\n",
            "Track generation: 1000..1263 -> 263-tiles track\n",
            " 2095/3000: episode: 117, duration: 5.072s, episode steps:  18, steps per second:   4, episode reward: -93.750, mean reward: -5.208 [-100.100,  7.334], mean action: 5.556 [0.000, 13.000],  loss: 128.225476, mae: 39.586567, mean_q: -39.114635, mean_eps: 0.812305\n",
            "Track generation: 1151..1443 -> 292-tiles track\n",
            " 2113/3000: episode: 118, duration: 5.402s, episode steps:  18, steps per second:   3, episode reward: -94.891, mean reward: -5.272 [-100.100,  6.573], mean action: 6.333 [1.000, 15.000],  loss: 103.380595, mae: 40.213410, mean_q: -39.063500, mean_eps: 0.810685\n",
            "Track generation: 1186..1496 -> 310-tiles track\n",
            " 2131/3000: episode: 119, duration: 5.121s, episode steps:  18, steps per second:   4, episode reward: -95.491, mean reward: -5.305 [-100.100,  6.172], mean action: 8.722 [2.000, 15.000],  loss: 91.709020, mae: 39.641147, mean_q: -39.144654, mean_eps: 0.809065\n",
            "Track generation: 1180..1479 -> 299-tiles track\n",
            " 2149/3000: episode: 120, duration: 5.052s, episode steps:  18, steps per second:   4, episode reward: -95.133, mean reward: -5.285 [-100.100,  6.411], mean action: 8.556 [0.000, 15.000],  loss: 118.518321, mae: 39.450999, mean_q: -38.570738, mean_eps: 0.807445\n",
            "Track generation: 1032..1294 -> 262-tiles track\n",
            " 2166/3000: episode: 121, duration: 4.818s, episode steps:  17, steps per second:   4, episode reward: -97.437, mean reward: -5.732 [-100.300,  7.363], mean action: 9.235 [2.000, 15.000],  loss: 106.860423, mae: 39.670994, mean_q: -38.610581, mean_eps: 0.805870\n",
            "Track generation: 1190..1492 -> 302-tiles track\n",
            " 2184/3000: episode: 122, duration: 5.073s, episode steps:  18, steps per second:   4, episode reward: -95.233, mean reward: -5.291 [-100.100,  6.345], mean action: 8.500 [0.000, 15.000],  loss: 99.453487, mae: 39.793575, mean_q: -39.114653, mean_eps: 0.804295\n",
            "Track generation: 1140..1429 -> 289-tiles track\n",
            " 2202/3000: episode: 123, duration: 5.384s, episode steps:  18, steps per second:   3, episode reward: -91.411, mean reward: -5.078 [-100.200,  6.644], mean action: 7.056 [0.000, 15.000],  loss: 118.427146, mae: 40.043784, mean_q: -39.643061, mean_eps: 0.802675\n",
            "Track generation: 1204..1509 -> 305-tiles track\n",
            " 2220/3000: episode: 124, duration: 5.115s, episode steps:  18, steps per second:   4, episode reward: -95.332, mean reward: -5.296 [-100.100,  6.279], mean action: 8.000 [1.000, 15.000],  loss: 72.552403, mae: 39.953000, mean_q: -39.852724, mean_eps: 0.801055\n",
            "Track generation: 1096..1380 -> 284-tiles track\n",
            " 2238/3000: episode: 125, duration: 5.124s, episode steps:  18, steps per second:   4, episode reward: -94.599, mean reward: -5.256 [-100.100,  6.767], mean action: 6.333 [1.000, 14.000],  loss: 112.729534, mae: 40.041037, mean_q: -40.166641, mean_eps: 0.799435\n",
            "Track generation: 1000..1259 -> 259-tiles track\n",
            " 2256/3000: episode: 126, duration: 5.068s, episode steps:  18, steps per second:   4, episode reward: -89.796, mean reward: -4.989 [-100.200,  7.452], mean action: 7.389 [2.000, 15.000],  loss: 98.200532, mae: 40.653177, mean_q: -40.227568, mean_eps: 0.797815\n",
            "Track generation: 1212..1525 -> 313-tiles track\n",
            " 2274/3000: episode: 127, duration: 5.010s, episode steps:  18, steps per second:   4, episode reward: -95.585, mean reward: -5.310 [-100.100,  6.110], mean action: 7.389 [1.000, 14.000],  loss: 118.257614, mae: 41.229917, mean_q: -41.050952, mean_eps: 0.796195\n",
            "Track generation: 1180..1479 -> 299-tiles track\n",
            " 2292/3000: episode: 128, duration: 5.057s, episode steps:  18, steps per second:   4, episode reward: -91.877, mean reward: -5.104 [-100.200,  6.411], mean action: 5.556 [0.000, 13.000],  loss: 95.821668, mae: 40.999285, mean_q: -40.400337, mean_eps: 0.794575\n",
            "Track generation: 1127..1417 -> 290-tiles track\n",
            "retry to generate track (normal if there are not manyinstances of this message)\n",
            "Track generation: 1253..1570 -> 317-tiles track\n",
            " 2310/3000: episode: 129, duration: 5.420s, episode steps:  18, steps per second:   3, episode reward: -95.706, mean reward: -5.317 [-100.100,  6.029], mean action: 8.500 [0.000, 14.000],  loss: 105.955044, mae: 40.434585, mean_q: -40.734147, mean_eps: 0.792955\n",
            "Track generation: 1084..1359 -> 275-tiles track\n",
            " 2328/3000: episode: 130, duration: 5.147s, episode steps:  18, steps per second:   3, episode reward: -90.701, mean reward: -5.039 [-100.200,  6.999], mean action: 6.778 [1.000, 15.000],  loss: 117.082404, mae: 41.097600, mean_q: -41.227399, mean_eps: 0.791335\n",
            "Track generation: 1240..1553 -> 313-tiles track\n",
            " 2346/3000: episode: 131, duration: 5.134s, episode steps:  18, steps per second:   4, episode reward: -92.479, mean reward: -5.138 [-100.200,  6.110], mean action: 5.111 [0.000, 15.000],  loss: 101.239835, mae: 41.611311, mean_q: -41.452451, mean_eps: 0.789715\n",
            "Track generation: 1104..1384 -> 280-tiles track\n",
            " 2364/3000: episode: 132, duration: 5.091s, episode steps:  18, steps per second:   4, episode reward: -87.479, mean reward: -4.860 [-100.300,  6.868], mean action: 5.444 [0.000, 14.000],  loss: 75.258663, mae: 41.378961, mean_q: -41.591962, mean_eps: 0.788095\n",
            "Track generation: 1117..1400 -> 283-tiles track\n",
            " 2382/3000: episode: 133, duration: 5.149s, episode steps:  18, steps per second:   3, episode reward: -91.116, mean reward: -5.062 [-100.200,  6.792], mean action: 4.611 [0.000, 15.000],  loss: 74.179237, mae: 40.739094, mean_q: -40.608548, mean_eps: 0.786475\n",
            "Track generation: 1204..1509 -> 305-tiles track\n",
            " 2400/3000: episode: 134, duration: 5.455s, episode steps:  18, steps per second:   3, episode reward: -92.142, mean reward: -5.119 [-96.911,  6.279], mean action: 5.389 [0.000, 15.000],  loss: 92.469443, mae: 40.460001, mean_q: -40.600509, mean_eps: 0.784855\n",
            "Track generation: 1167..1463 -> 296-tiles track\n",
            " 2417/3000: episode: 135, duration: 5.162s, episode steps:  17, steps per second:   3, episode reward: -98.320, mean reward: -5.784 [-100.300,  6.480], mean action: 10.118 [0.000, 15.000],  loss: 87.384659, mae: 40.593149, mean_q: -40.800151, mean_eps: 0.783280\n",
            "Track generation: 1052..1319 -> 267-tiles track\n",
            " 2434/3000: episode: 136, duration: 4.869s, episode steps:  17, steps per second:   3, episode reward: -97.581, mean reward: -5.740 [-100.300,  7.219], mean action: 8.882 [0.000, 15.000],  loss: 85.636000, mae: 40.732943, mean_q: -41.047776, mean_eps: 0.781750\n",
            "Track generation: 1140..1429 -> 289-tiles track\n",
            " 2451/3000: episode: 137, duration: 4.884s, episode steps:  17, steps per second:   3, episode reward: -98.156, mean reward: -5.774 [-100.300,  6.644], mean action: 9.706 [5.000, 15.000],  loss: 114.241402, mae: 40.933988, mean_q: -40.778430, mean_eps: 0.780220\n",
            "Track generation: 1137..1425 -> 288-tiles track\n",
            " 2469/3000: episode: 138, duration: 5.108s, episode steps:  18, steps per second:   4, episode reward: -94.747, mean reward: -5.264 [-100.100,  6.669], mean action: 8.333 [0.000, 15.000],  loss: 105.827580, mae: 41.638274, mean_q: -41.557025, mean_eps: 0.778645\n",
            "Track generation: 905..1141 -> 236-tiles track\n",
            " 2487/3000: episode: 139, duration: 5.139s, episode steps:  18, steps per second:   4, episode reward: -92.434, mean reward: -5.135 [-100.100,  8.211], mean action: 9.000 [2.000, 15.000],  loss: 85.209708, mae: 41.449923, mean_q: -41.086647, mean_eps: 0.777025\n",
            "Track generation: 1131..1418 -> 287-tiles track\n",
            " 2505/3000: episode: 140, duration: 5.370s, episode steps:  18, steps per second:   3, episode reward: -94.710, mean reward: -5.262 [-100.100,  6.693], mean action: 9.111 [0.000, 15.000],  loss: 106.253149, mae: 40.795285, mean_q: -40.596296, mean_eps: 0.775405\n",
            "Track generation: 1171..1468 -> 297-tiles track\n",
            " 2522/3000: episode: 141, duration: 4.853s, episode steps:  17, steps per second:   4, episode reward: -98.343, mean reward: -5.785 [-100.300,  6.457], mean action: 9.471 [3.000, 15.000],  loss: 95.215919, mae: 41.147735, mean_q: -41.097243, mean_eps: 0.773830\n",
            "Track generation: 1131..1418 -> 287-tiles track\n",
            " 2540/3000: episode: 142, duration: 5.132s, episode steps:  18, steps per second:   4, episode reward: -94.710, mean reward: -5.262 [-100.100,  6.693], mean action: 7.667 [0.000, 15.000],  loss: 102.296344, mae: 41.239799, mean_q: -41.086584, mean_eps: 0.772255\n",
            "Track generation: 1137..1425 -> 288-tiles track\n",
            " 2558/3000: episode: 143, duration: 5.121s, episode steps:  18, steps per second:   4, episode reward: -94.747, mean reward: -5.264 [-100.100,  6.669], mean action: 7.167 [0.000, 15.000],  loss: 105.491724, mae: 41.358691, mean_q: -41.460001, mean_eps: 0.770635\n",
            "Track generation: 1107..1388 -> 281-tiles track\n",
            " 2575/3000: episode: 144, duration: 4.867s, episode steps:  17, steps per second:   3, episode reward: -97.957, mean reward: -5.762 [-100.300,  6.843], mean action: 9.353 [2.000, 15.000],  loss: 94.404266, mae: 41.178889, mean_q: -41.973450, mean_eps: 0.769060\n",
            "Track generation: 1307..1638 -> 331-tiles track\n",
            " 2593/3000: episode: 145, duration: 5.143s, episode steps:  18, steps per second:   4, episode reward: -90.248, mean reward: -5.014 [-100.300,  5.761], mean action: 6.056 [2.000, 14.000],  loss: 98.862285, mae: 41.141806, mean_q: -41.402672, mean_eps: 0.767485\n",
            "Track generation: 1195..1497 -> 302-tiles track\n",
            " 2611/3000: episode: 146, duration: 5.403s, episode steps:  18, steps per second:   3, episode reward: -95.233, mean reward: -5.291 [-100.100,  6.345], mean action: 5.778 [0.000, 14.000],  loss: 89.680851, mae: 41.095351, mean_q: -41.305091, mean_eps: 0.765865\n",
            "Track generation: 1170..1467 -> 297-tiles track\n",
            " 2629/3000: episode: 147, duration: 5.096s, episode steps:  18, steps per second:   4, episode reward: -95.065, mean reward: -5.281 [-100.100,  6.457], mean action: 7.333 [0.000, 13.000],  loss: 90.775499, mae: 41.241314, mean_q: -40.990130, mean_eps: 0.764245\n",
            "Track generation: 1172..1469 -> 297-tiles track\n",
            " 2647/3000: episode: 148, duration: 5.114s, episode steps:  18, steps per second:   4, episode reward: -95.065, mean reward: -5.281 [-100.100,  6.457], mean action: 7.833 [0.000, 15.000],  loss: 75.054045, mae: 41.455947, mean_q: -41.803744, mean_eps: 0.762625\n",
            "Track generation: 1120..1404 -> 284-tiles track\n",
            " 2665/3000: episode: 149, duration: 5.126s, episode steps:  18, steps per second:   4, episode reward: -94.599, mean reward: -5.256 [-100.100,  6.767], mean action: 9.111 [0.000, 15.000],  loss: 93.217840, mae: 41.529813, mean_q: -41.558802, mean_eps: 0.761005\n",
            "Track generation: 1001..1259 -> 258-tiles track\n",
            "retry to generate track (normal if there are not manyinstances of this message)\n",
            "Track generation: 1192..1493 -> 301-tiles track\n",
            " 2683/3000: episode: 150, duration: 5.108s, episode steps:  18, steps per second:   4, episode reward: -95.200, mean reward: -5.289 [-100.100,  6.367], mean action: 7.278 [0.000, 14.000],  loss: 116.190395, mae: 41.874851, mean_q: -41.451947, mean_eps: 0.759385\n",
            "Track generation: 1089..1364 -> 275-tiles track\n",
            " 2701/3000: episode: 151, duration: 5.393s, episode steps:  18, steps per second:   3, episode reward: -94.251, mean reward: -5.236 [-100.100,  6.999], mean action: 7.556 [0.000, 13.000],  loss: 108.374836, mae: 42.359869, mean_q: -42.390780, mean_eps: 0.757765\n",
            "Track generation: 1245..1560 -> 315-tiles track\n",
            " 2719/3000: episode: 152, duration: 5.108s, episode steps:  18, steps per second:   4, episode reward: -95.646, mean reward: -5.314 [-100.100,  6.069], mean action: 6.889 [0.000, 15.000],  loss: 95.099071, mae: 42.310921, mean_q: -43.082830, mean_eps: 0.756145\n",
            "Track generation: 1097..1382 -> 285-tiles track\n",
            " 2737/3000: episode: 153, duration: 5.090s, episode steps:  18, steps per second:   4, episode reward: -91.215, mean reward: -5.068 [-100.200,  6.742], mean action: 7.167 [0.000, 15.000],  loss: 95.149060, mae: 43.115861, mean_q: -44.082565, mean_eps: 0.754525\n",
            "Track generation: 1237..1554 -> 317-tiles track\n",
            "retry to generate track (normal if there are not manyinstances of this message)\n",
            "Track generation: 1175..1473 -> 298-tiles track\n",
            " 2755/3000: episode: 154, duration: 5.052s, episode steps:  18, steps per second:   4, episode reward: -95.099, mean reward: -5.283 [-100.100,  6.434], mean action: 7.722 [0.000, 15.000],  loss: 79.850488, mae: 43.092941, mean_q: -43.496433, mean_eps: 0.752905\n",
            "Track generation: 1140..1435 -> 295-tiles track\n",
            " 2773/3000: episode: 155, duration: 5.048s, episode steps:  18, steps per second:   4, episode reward: -94.996, mean reward: -5.278 [-100.100,  6.503], mean action: 8.556 [1.000, 15.000],  loss: 88.363139, mae: 42.800791, mean_q: -42.469279, mean_eps: 0.751285\n",
            "Track generation: 1128..1421 -> 293-tiles track\n",
            " 2790/3000: episode: 156, duration: 4.796s, episode steps:  17, steps per second:   4, episode reward: -98.251, mean reward: -5.779 [-100.300,  6.549], mean action: 11.059 [0.000, 15.000],  loss: 79.421845, mae: 42.709079, mean_q: -42.646208, mean_eps: 0.749710\n",
            "Track generation: 1162..1456 -> 294-tiles track\n",
            " 2808/3000: episode: 157, duration: 5.282s, episode steps:  18, steps per second:   3, episode reward: -94.961, mean reward: -5.276 [-100.100,  6.526], mean action: 9.167 [0.000, 15.000],  loss: 101.400917, mae: 42.695352, mean_q: -42.666588, mean_eps: 0.748135\n",
            "Track generation: 1190..1491 -> 301-tiles track\n",
            " 2826/3000: episode: 158, duration: 5.038s, episode steps:  18, steps per second:   4, episode reward: -95.200, mean reward: -5.289 [-100.100,  6.367], mean action: 8.556 [0.000, 15.000],  loss: 90.707850, mae: 43.135574, mean_q: -42.865417, mean_eps: 0.746515\n",
            "Track generation: 1185..1485 -> 300-tiles track\n",
            " 2844/3000: episode: 159, duration: 5.059s, episode steps:  18, steps per second:   4, episode reward: -95.167, mean reward: -5.287 [-100.100,  6.389], mean action: 7.056 [1.000, 15.000],  loss: 82.910230, mae: 42.738203, mean_q: -42.356194, mean_eps: 0.744895\n",
            "Track generation: 1160..1454 -> 294-tiles track\n",
            " 2862/3000: episode: 160, duration: 5.082s, episode steps:  18, steps per second:   4, episode reward: -88.335, mean reward: -4.908 [-100.300,  6.526], mean action: 6.278 [0.000, 14.000],  loss: 109.905016, mae: 43.233502, mean_q: -43.137630, mean_eps: 0.743275\n",
            "Track generation: 1171..1471 -> 300-tiles track\n",
            "retry to generate track (normal if there are not manyinstances of this message)\n",
            "Track generation: 1120..1404 -> 284-tiles track\n",
            " 2880/3000: episode: 161, duration: 5.089s, episode steps:  18, steps per second:   4, episode reward: -87.732, mean reward: -4.874 [-96.766,  6.767], mean action: 4.278 [1.000, 14.000],  loss: 92.133177, mae: 43.169665, mean_q: -43.315321, mean_eps: 0.741655\n",
            "Track generation: 1061..1336 -> 275-tiles track\n",
            " 2898/3000: episode: 162, duration: 5.099s, episode steps:  18, steps per second:   4, episode reward: -94.251, mean reward: -5.236 [-100.100,  6.999], mean action: 8.056 [0.000, 15.000],  loss: 78.695152, mae: 42.740006, mean_q: -43.089769, mean_eps: 0.740035\n",
            "Track generation: 1027..1288 -> 261-tiles track\n",
            " 2916/3000: episode: 163, duration: 5.338s, episode steps:  18, steps per second:   3, episode reward: -93.662, mean reward: -5.203 [-100.100,  7.392], mean action: 6.889 [0.000, 15.000],  loss: 96.421770, mae: 42.790255, mean_q: -43.003718, mean_eps: 0.738415\n",
            "Track generation: 1147..1438 -> 291-tiles track\n",
            " 2934/3000: episode: 164, duration: 5.073s, episode steps:  18, steps per second:   4, episode reward: -94.855, mean reward: -5.270 [-100.100,  6.597], mean action: 7.611 [0.000, 13.000],  loss: 104.796660, mae: 43.215081, mean_q: -43.252867, mean_eps: 0.736795\n",
            "Track generation: 983..1239 -> 256-tiles track\n",
            " 2951/3000: episode: 165, duration: 4.805s, episode steps:  17, steps per second:   4, episode reward: -97.257, mean reward: -5.721 [-100.300,  7.543], mean action: 12.235 [3.000, 15.000],  loss: 93.931490, mae: 43.432011, mean_q: -42.810029, mean_eps: 0.735220\n",
            "Track generation: 1149..1440 -> 291-tiles track\n",
            " 2969/3000: episode: 166, duration: 5.088s, episode steps:  18, steps per second:   4, episode reward: -94.855, mean reward: -5.270 [-100.100,  6.597], mean action: 8.278 [0.000, 15.000],  loss: 103.664363, mae: 43.316783, mean_q: -42.060730, mean_eps: 0.733645\n",
            "Track generation: 1152..1444 -> 292-tiles track\n",
            " 2987/3000: episode: 167, duration: 5.103s, episode steps:  18, steps per second:   4, episode reward: -94.891, mean reward: -5.272 [-100.100,  6.573], mean action: 8.667 [0.000, 15.000],  loss: 64.283186, mae: 43.375948, mean_q: -41.973985, mean_eps: 0.732025\n",
            "Track generation: 1109..1390 -> 281-tiles track\n",
            "done, took 746.135 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff0f9fb5f28>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcZ97L1stXxg"
      },
      "source": [
        " # After training is done, we save the final weights one more time.\n",
        "dqn.save_weights(weights_filename, overwrite=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fieLYJUtXxj",
        "outputId": "74ee0be5-41d5-4425-a2dc-6647b7f4e1d3"
      },
      "source": [
        "# Finally, evaluate our algorithm for 10 episodes.\n",
        "dqn.test(env, nb_episodes=10, visualize=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing for 10 episodes ...\n",
            "Track generation: 1286..1614 -> 328-tiles track\n",
            "retry to generate track (normal if there are not manyinstances of this message)\n",
            "Track generation: 1252..1569 -> 317-tiles track\n",
            "Episode 1: reward: -86.513, steps: 55\n",
            "Track generation: 1296..1624 -> 328-tiles track\n",
            "Episode 2: reward: -87.151, steps: 55\n",
            "Track generation: 1148..1444 -> 296-tiles track\n",
            "Episode 3: reward: -85.161, steps: 55\n",
            "Track generation: 1084..1359 -> 275-tiles track\n",
            "Episode 4: reward: -83.602, steps: 55\n",
            "Track generation: 1256..1574 -> 318-tiles track\n",
            "Episode 5: reward: -86.573, steps: 55\n",
            "Track generation: 1094..1377 -> 283-tiles track\n",
            "Episode 6: reward: -84.223, steps: 55\n",
            "Track generation: 1210..1516 -> 306-tiles track\n",
            "Episode 7: reward: -85.828, steps: 55\n",
            "Track generation: 1268..1589 -> 321-tiles track\n",
            "Episode 8: reward: -86.750, steps: 55\n",
            "Track generation: 1124..1409 -> 285-tiles track\n",
            "Episode 9: reward: -84.373, steps: 55\n",
            "Track generation: 1206..1523 -> 317-tiles track\n",
            "Episode 10: reward: -86.513, steps: 55\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff099a17dd8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fL1CvMfyOXfM",
        "outputId": "418782f2-ed64-4346-cce7-68cd0183f771"
      },
      "source": [
        "for i in range(10):\r\n",
        "  dqn.load_weights(weights_filename)\r\n",
        "  dqn.fit(env, nb_steps=3000, verbose=2, nb_max_episode_steps=200, action_repetition=3, visualize = False)\r\n",
        "  dqn.save_weights(weights_filename, overwrite=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for 3000 steps ...\n",
            "Track generation: 1004..1262 -> 258-tiles track\n",
            "retry to generate track (normal if there are not manyinstances of this message)\n",
            "Track generation: 1163..1462 -> 299-tiles track\n",
            "   18/3000: episode: 1, duration: 0.784s, episode steps:  18, steps per second:  23, episode reward: -95.133, mean reward: -5.285 [-100.100,  6.411], mean action: 7.667 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1208..1514 -> 306-tiles track\n",
            "   36/3000: episode: 2, duration: 0.488s, episode steps:  18, steps per second:  37, episode reward: -95.364, mean reward: -5.298 [-100.100,  6.257], mean action: 7.111 [1.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1216..1524 -> 308-tiles track\n",
            "   54/3000: episode: 3, duration: 0.492s, episode steps:  18, steps per second:  37, episode reward: -95.428, mean reward: -5.302 [-100.100,  6.215], mean action: 6.833 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1064..1334 -> 270-tiles track\n",
            "   72/3000: episode: 4, duration: 0.487s, episode steps:  18, steps per second:  37, episode reward: -94.048, mean reward: -5.225 [-100.100,  7.135], mean action: 6.556 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1103..1391 -> 288-tiles track\n",
            "   90/3000: episode: 5, duration: 0.488s, episode steps:  18, steps per second:  37, episode reward: -94.747, mean reward: -5.264 [-100.100,  6.669], mean action: 6.944 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1187..1488 -> 301-tiles track\n",
            "  108/3000: episode: 6, duration: 0.781s, episode steps:  18, steps per second:  23, episode reward: -95.200, mean reward: -5.289 [-100.100,  6.367], mean action: 8.111 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1079..1353 -> 274-tiles track\n",
            "  126/3000: episode: 7, duration: 0.486s, episode steps:  18, steps per second:  37, episode reward: -94.211, mean reward: -5.234 [-100.100,  7.026], mean action: 9.278 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 995..1257 -> 262-tiles track\n",
            "  144/3000: episode: 8, duration: 0.502s, episode steps:  18, steps per second:  36, episode reward: -89.974, mean reward: -4.999 [-100.200,  7.363], mean action: 6.444 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1043..1308 -> 265-tiles track\n",
            "  162/3000: episode: 9, duration: 0.476s, episode steps:  18, steps per second:  38, episode reward: -93.836, mean reward: -5.213 [-100.100,  7.276], mean action: 7.500 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1182..1488 -> 306-tiles track\n",
            "  180/3000: episode: 10, duration: 0.494s, episode steps:  18, steps per second:  36, episode reward: -95.364, mean reward: -5.298 [-100.100,  6.257], mean action: 6.667 [1.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1309..1640 -> 331-tiles track\n",
            "  198/3000: episode: 11, duration: 0.512s, episode steps:  18, steps per second:  35, episode reward: -96.109, mean reward: -5.339 [-100.100,  5.761], mean action: 8.333 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1200..1504 -> 304-tiles track\n",
            "  216/3000: episode: 12, duration: 0.803s, episode steps:  18, steps per second:  22, episode reward: -88.898, mean reward: -4.939 [-100.300,  6.301], mean action: 5.556 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1056..1324 -> 268-tiles track\n",
            "  234/3000: episode: 13, duration: 0.500s, episode steps:  18, steps per second:  36, episode reward: -93.964, mean reward: -5.220 [-100.100,  7.191], mean action: 7.722 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1267..1588 -> 321-tiles track\n",
            "  252/3000: episode: 14, duration: 0.512s, episode steps:  18, steps per second:  35, episode reward: -95.825, mean reward: -5.324 [-100.100,  5.950], mean action: 7.944 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1269..1590 -> 321-tiles track\n",
            "  270/3000: episode: 15, duration: 0.502s, episode steps:  18, steps per second:  36, episode reward: -92.800, mean reward: -5.156 [-100.200,  5.950], mean action: 6.444 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1051..1320 -> 269-tiles track\n",
            "retry to generate track (normal if there are not manyinstances of this message)\n",
            "Track generation: 1013..1278 -> 265-tiles track\n",
            "  288/3000: episode: 16, duration: 0.496s, episode steps:  18, steps per second:  36, episode reward: -93.836, mean reward: -5.213 [-100.100,  7.276], mean action: 8.778 [1.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1240..1554 -> 314-tiles track\n",
            "  306/3000: episode: 17, duration: 0.785s, episode steps:  18, steps per second:  23, episode reward: -92.520, mean reward: -5.140 [-100.200,  6.090], mean action: 5.500 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1047..1313 -> 266-tiles track\n",
            "  324/3000: episode: 18, duration: 0.504s, episode steps:  18, steps per second:  36, episode reward: -93.879, mean reward: -5.216 [-100.100,  7.247], mean action: 7.111 [1.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1247..1563 -> 316-tiles track\n",
            "  342/3000: episode: 19, duration: 0.514s, episode steps:  18, steps per second:  35, episode reward: -89.527, mean reward: -4.974 [-97.125,  6.049], mean action: 5.444 [1.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1280..1604 -> 324-tiles track\n",
            "  360/3000: episode: 20, duration: 0.497s, episode steps:  18, steps per second:  36, episode reward: -95.912, mean reward: -5.328 [-100.100,  5.892], mean action: 7.667 [1.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1148..1442 -> 294-tiles track\n",
            "retry to generate track (normal if there are not manyinstances of this message)\n",
            "Track generation: 1163..1466 -> 303-tiles track\n",
            "  378/3000: episode: 21, duration: 0.499s, episode steps:  18, steps per second:  36, episode reward: -95.266, mean reward: -5.293 [-100.100,  6.323], mean action: 8.611 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1208..1514 -> 306-tiles track\n",
            "  396/3000: episode: 22, duration: 0.501s, episode steps:  18, steps per second:  36, episode reward: -95.364, mean reward: -5.298 [-100.100,  6.257], mean action: 7.667 [1.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1104..1391 -> 287-tiles track\n",
            "  414/3000: episode: 23, duration: 0.782s, episode steps:  18, steps per second:  23, episode reward: -94.710, mean reward: -5.262 [-100.100,  6.693], mean action: 7.778 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1275..1598 -> 323-tiles track\n",
            "  432/3000: episode: 24, duration: 0.517s, episode steps:  18, steps per second:  35, episode reward: -95.883, mean reward: -5.327 [-100.100,  5.911], mean action: 7.722 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1076..1349 -> 273-tiles track\n",
            "  450/3000: episode: 25, duration: 0.515s, episode steps:  18, steps per second:  35, episode reward: -94.171, mean reward: -5.232 [-100.100,  7.053], mean action: 7.722 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1100..1379 -> 279-tiles track\n",
            "  468/3000: episode: 26, duration: 0.502s, episode steps:  18, steps per second:  36, episode reward: -94.409, mean reward: -5.245 [-100.100,  6.894], mean action: 5.722 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1157..1453 -> 296-tiles track\n",
            "retry to generate track (normal if there are not manyinstances of this message)\n",
            "Track generation: 1288..1614 -> 326-tiles track\n",
            "  486/3000: episode: 27, duration: 0.517s, episode steps:  18, steps per second:  35, episode reward: -95.969, mean reward: -5.332 [-100.100,  5.854], mean action: 8.000 [1.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1080..1355 -> 275-tiles track\n",
            "retry to generate track (normal if there are not manyinstances of this message)\n",
            "Track generation: 899..1131 -> 232-tiles track\n",
            "retry to generate track (normal if there are not manyinstances of this message)\n",
            "Track generation: 1112..1394 -> 282-tiles track\n",
            "  504/3000: episode: 28, duration: 1.767s, episode steps:  18, steps per second:  10, episode reward: -94.524, mean reward: -5.251 [-100.100,  6.817], mean action: 7.278 [0.000, 15.000],  loss: 37.279022, mae: 42.585405, mean_q: -42.176266, mean_eps: 0.954820\n",
            "Track generation: 1103..1377 -> 274-tiles track\n",
            "  521/3000: episode: 29, duration: 4.880s, episode steps:  17, steps per second:   3, episode reward: -97.774, mean reward: -5.751 [-100.300,  7.026], mean action: 9.118 [0.000, 15.000],  loss: 67.394466, mae: 42.202443, mean_q: -41.846051, mean_eps: 0.953920\n",
            "Track generation: 952..1201 -> 249-tiles track\n",
            "  539/3000: episode: 30, duration: 7.591s, episode steps:  18, steps per second:   2, episode reward: -93.103, mean reward: -5.172 [-100.100,  7.765], mean action: 6.778 [0.000, 14.000],  loss: 103.134884, mae: 41.829879, mean_q: -41.879092, mean_eps: 0.952345\n",
            "Track generation: 1023..1283 -> 260-tiles track\n",
            "  557/3000: episode: 31, duration: 5.031s, episode steps:  18, steps per second:   4, episode reward: -89.856, mean reward: -4.992 [-100.200,  7.422], mean action: 7.000 [0.000, 15.000],  loss: 84.735465, mae: 42.262490, mean_q: -42.382201, mean_eps: 0.950725\n",
            "Track generation: 1044..1309 -> 265-tiles track\n",
            "  575/3000: episode: 32, duration: 5.014s, episode steps:  18, steps per second:   4, episode reward: -93.836, mean reward: -5.213 [-100.100,  7.276], mean action: 9.167 [0.000, 15.000],  loss: 111.025790, mae: 42.516438, mean_q: -42.963241, mean_eps: 0.949105\n",
            "Track generation: 1345..1686 -> 341-tiles track\n",
            "  592/3000: episode: 33, duration: 4.780s, episode steps:  17, steps per second:   4, episode reward: -99.218, mean reward: -5.836 [-100.300,  5.582], mean action: 8.824 [0.000, 14.000],  loss: 95.075574, mae: 42.758698, mean_q: -43.749487, mean_eps: 0.947530\n",
            "Track generation: 1100..1379 -> 279-tiles track\n",
            "  610/3000: episode: 34, duration: 5.313s, episode steps:  18, steps per second:   3, episode reward: -94.409, mean reward: -5.245 [-100.100,  6.894], mean action: 7.500 [0.000, 15.000],  loss: 88.243369, mae: 42.525132, mean_q: -43.253140, mean_eps: 0.945955\n",
            "Track generation: 1208..1504 -> 296-tiles track\n",
            "  628/3000: episode: 35, duration: 5.020s, episode steps:  18, steps per second:   4, episode reward: -91.741, mean reward: -5.097 [-100.200,  6.480], mean action: 6.444 [0.000, 14.000],  loss: 106.721142, mae: 42.734527, mean_q: -43.196497, mean_eps: 0.944335\n",
            "Track generation: 1200..1503 -> 303-tiles track\n",
            "  646/3000: episode: 36, duration: 5.054s, episode steps:  18, steps per second:   4, episode reward: -95.266, mean reward: -5.293 [-100.100,  6.323], mean action: 8.833 [1.000, 14.000],  loss: 100.222624, mae: 43.811222, mean_q: -44.353097, mean_eps: 0.942715\n",
            "Track generation: 1156..1449 -> 293-tiles track\n",
            "  664/3000: episode: 37, duration: 5.048s, episode steps:  18, steps per second:   4, episode reward: -94.926, mean reward: -5.274 [-100.100,  6.549], mean action: 8.000 [0.000, 15.000],  loss: 73.841731, mae: 43.457940, mean_q: -43.828239, mean_eps: 0.941095\n",
            "Track generation: 1195..1498 -> 303-tiles track\n",
            "  682/3000: episode: 38, duration: 5.004s, episode steps:  18, steps per second:   4, episode reward: -95.266, mean reward: -5.293 [-100.100,  6.323], mean action: 6.056 [0.000, 13.000],  loss: 86.372425, mae: 43.098002, mean_q: -43.803159, mean_eps: 0.939475\n",
            "Track generation: 1184..1485 -> 301-tiles track\n",
            "  700/3000: episode: 39, duration: 5.311s, episode steps:  18, steps per second:   3, episode reward: -95.200, mean reward: -5.289 [-100.100,  6.367], mean action: 7.889 [1.000, 15.000],  loss: 105.988275, mae: 43.432146, mean_q: -43.112167, mean_eps: 0.937855\n",
            "Track generation: 1224..1534 -> 310-tiles track\n",
            "  717/3000: episode: 40, duration: 5.138s, episode steps:  17, steps per second:   3, episode reward: -98.628, mean reward: -5.802 [-100.300,  6.172], mean action: 8.000 [1.000, 15.000],  loss: 111.715726, mae: 43.728411, mean_q: -42.610774, mean_eps: 0.936280\n",
            "Track generation: 1014..1275 -> 261-tiles track\n",
            "retry to generate track (normal if there are not manyinstances of this message)\n",
            "Track generation: 1057..1330 -> 273-tiles track\n",
            "  735/3000: episode: 41, duration: 5.095s, episode steps:  18, steps per second:   4, episode reward: -90.594, mean reward: -5.033 [-100.200,  7.053], mean action: 6.444 [1.000, 15.000],  loss: 84.103382, mae: 43.632338, mean_q: -42.306202, mean_eps: 0.934705\n",
            "Track generation: 1112..1394 -> 282-tiles track\n",
            "  753/3000: episode: 42, duration: 5.083s, episode steps:  18, steps per second:   4, episode reward: -94.524, mean reward: -5.251 [-100.100,  6.817], mean action: 7.611 [1.000, 15.000],  loss: 72.418577, mae: 42.603374, mean_q: -41.979589, mean_eps: 0.933085\n",
            "Track generation: 1160..1454 -> 294-tiles track\n",
            "  771/3000: episode: 43, duration: 5.093s, episode steps:  18, steps per second:   4, episode reward: -94.961, mean reward: -5.276 [-100.100,  6.526], mean action: 7.889 [0.000, 15.000],  loss: 94.595613, mae: 42.384549, mean_q: -42.628011, mean_eps: 0.931465\n",
            "Track generation: 1241..1555 -> 314-tiles track\n",
            "  789/3000: episode: 44, duration: 5.090s, episode steps:  18, steps per second:   4, episode reward: -95.615, mean reward: -5.312 [-100.100,  6.090], mean action: 7.056 [0.000, 15.000],  loss: 87.750844, mae: 42.755493, mean_q: -43.180892, mean_eps: 0.929845\n",
            "Track generation: 1207..1513 -> 306-tiles track\n",
            "  807/3000: episode: 45, duration: 5.358s, episode steps:  18, steps per second:   3, episode reward: -95.364, mean reward: -5.298 [-100.100,  6.257], mean action: 8.556 [1.000, 14.000],  loss: 71.494855, mae: 42.655919, mean_q: -42.841938, mean_eps: 0.928225\n",
            "Track generation: 1048..1317 -> 269-tiles track\n",
            "retry to generate track (normal if there are not manyinstances of this message)\n",
            "Track generation: 1187..1488 -> 301-tiles track\n",
            "  825/3000: episode: 46, duration: 5.089s, episode steps:  18, steps per second:   4, episode reward: -95.200, mean reward: -5.289 [-100.100,  6.367], mean action: 8.444 [3.000, 15.000],  loss: 123.313680, mae: 42.772696, mean_q: -42.623001, mean_eps: 0.926605\n",
            "Track generation: 1316..1649 -> 333-tiles track\n",
            "  842/3000: episode: 47, duration: 4.840s, episode steps:  17, steps per second:   4, episode reward: -99.076, mean reward: -5.828 [-100.300,  5.724], mean action: 8.941 [1.000, 15.000],  loss: 78.559044, mae: 43.487173, mean_q: -43.651213, mean_eps: 0.925030\n",
            "Track generation: 1124..1409 -> 285-tiles track\n",
            "  860/3000: episode: 48, duration: 5.076s, episode steps:  18, steps per second:   4, episode reward: -91.215, mean reward: -5.068 [-100.200,  6.742], mean action: 7.389 [0.000, 15.000],  loss: 110.851955, mae: 43.196282, mean_q: -43.393836, mean_eps: 0.923455\n",
            "Track generation: 1086..1370 -> 284-tiles track\n",
            "  878/3000: episode: 49, duration: 5.117s, episode steps:  18, steps per second:   4, episode reward: -87.732, mean reward: -4.874 [-100.300,  6.767], mean action: 5.944 [1.000, 15.000],  loss: 112.535687, mae: 43.653143, mean_q: -43.774747, mean_eps: 0.921835\n",
            "Track generation: 1140..1429 -> 289-tiles track\n",
            "  896/3000: episode: 50, duration: 5.108s, episode steps:  18, steps per second:   4, episode reward: -94.783, mean reward: -5.266 [-100.100,  6.644], mean action: 7.722 [0.000, 15.000],  loss: 103.441184, mae: 43.983615, mean_q: -44.363384, mean_eps: 0.920215\n",
            "Track generation: 1095..1373 -> 278-tiles track\n",
            "  913/3000: episode: 51, duration: 5.140s, episode steps:  17, steps per second:   3, episode reward: -97.880, mean reward: -5.758 [-100.300,  6.920], mean action: 9.176 [0.000, 15.000],  loss: 85.348952, mae: 43.570786, mean_q: -43.041758, mean_eps: 0.918640\n",
            "Track generation: 1184..1484 -> 300-tiles track\n",
            "  931/3000: episode: 52, duration: 5.085s, episode steps:  18, steps per second:   4, episode reward: -95.167, mean reward: -5.287 [-100.100,  6.389], mean action: 6.444 [0.000, 14.000],  loss: 88.417637, mae: 43.090039, mean_q: -42.961822, mean_eps: 0.917065\n",
            "Track generation: 1192..1494 -> 302-tiles track\n",
            "  949/3000: episode: 53, duration: 5.129s, episode steps:  18, steps per second:   4, episode reward: -95.233, mean reward: -5.291 [-100.100,  6.345], mean action: 8.000 [0.000, 15.000],  loss: 72.151606, mae: 42.911612, mean_q: -42.982412, mean_eps: 0.915445\n",
            "Track generation: 1267..1586 -> 319-tiles track\n",
            "  966/3000: episode: 54, duration: 4.863s, episode steps:  17, steps per second:   3, episode reward: -98.811, mean reward: -5.812 [-100.300,  5.989], mean action: 9.647 [1.000, 15.000],  loss: 97.196967, mae: 42.813106, mean_q: -42.805097, mean_eps: 0.913870\n",
            "Track generation: 1206..1516 -> 310-tiles track\n",
            "  984/3000: episode: 55, duration: 5.136s, episode steps:  18, steps per second:   4, episode reward: -95.491, mean reward: -5.305 [-100.100,  6.172], mean action: 6.222 [0.000, 14.000],  loss: 93.588669, mae: 43.431225, mean_q: -43.931871, mean_eps: 0.912295\n",
            "Track generation: 1036..1299 -> 263-tiles track\n",
            " 1002/3000: episode: 56, duration: 5.373s, episode steps:  18, steps per second:   3, episode reward: -93.750, mean reward: -5.208 [-100.100,  7.334], mean action: 6.778 [0.000, 15.000],  loss: 91.906196, mae: 43.482872, mean_q: -43.760309, mean_eps: 0.910675\n",
            "Track generation: 1244..1559 -> 315-tiles track\n",
            " 1019/3000: episode: 57, duration: 4.830s, episode steps:  17, steps per second:   4, episode reward: -98.731, mean reward: -5.808 [-100.300,  6.069], mean action: 9.588 [0.000, 14.000],  loss: 86.658904, mae: 43.249246, mean_q: -43.284252, mean_eps: 0.909100\n",
            "Track generation: 1136..1424 -> 288-tiles track\n",
            " 1037/3000: episode: 58, duration: 5.114s, episode steps:  18, steps per second:   4, episode reward: -91.363, mean reward: -5.076 [-100.200,  6.669], mean action: 5.944 [0.000, 13.000],  loss: 75.372049, mae: 43.209825, mean_q: -43.846849, mean_eps: 0.907525\n",
            "Track generation: 1048..1321 -> 273-tiles track\n",
            "retry to generate track (normal if there are not manyinstances of this message)\n",
            "Track generation: 901..1135 -> 234-tiles track\n",
            " 1055/3000: episode: 59, duration: 5.065s, episode steps:  18, steps per second:   4, episode reward: -92.324, mean reward: -5.129 [-100.100,  8.284], mean action: 7.000 [1.000, 15.000],  loss: 83.517208, mae: 43.096216, mean_q: -44.304619, mean_eps: 0.905905\n",
            "Track generation: 1046..1312 -> 266-tiles track\n",
            " 1073/3000: episode: 60, duration: 5.092s, episode steps:  18, steps per second:   4, episode reward: -90.206, mean reward: -5.011 [-100.200,  7.247], mean action: 7.000 [0.000, 15.000],  loss: 62.011721, mae: 42.936612, mean_q: -44.242406, mean_eps: 0.904285\n",
            "Track generation: 996..1258 -> 262-tiles track\n",
            " 1091/3000: episode: 61, duration: 5.099s, episode steps:  18, steps per second:   4, episode reward: -93.706, mean reward: -5.206 [-100.100,  7.363], mean action: 8.111 [0.000, 15.000],  loss: 85.586649, mae: 43.070505, mean_q: -43.744731, mean_eps: 0.902665\n",
            "Track generation: 1106..1395 -> 289-tiles track\n",
            " 1109/3000: episode: 62, duration: 5.350s, episode steps:  18, steps per second:   3, episode reward: -94.783, mean reward: -5.266 [-100.100,  6.644], mean action: 7.667 [0.000, 13.000],  loss: 98.689338, mae: 43.233290, mean_q: -43.158122, mean_eps: 0.901045\n",
            "Track generation: 1063..1343 -> 280-tiles track\n",
            " 1126/3000: episode: 63, duration: 4.801s, episode steps:  17, steps per second:   4, episode reward: -97.932, mean reward: -5.761 [-100.300,  6.868], mean action: 8.882 [1.000, 15.000],  loss: 74.337846, mae: 43.271357, mean_q: -42.833866, mean_eps: 0.899470\n",
            "Track generation: 1143..1433 -> 290-tiles track\n",
            " 1144/3000: episode: 64, duration: 5.039s, episode steps:  18, steps per second:   4, episode reward: -94.819, mean reward: -5.268 [-100.100,  6.620], mean action: 6.556 [2.000, 15.000],  loss: 82.233466, mae: 43.012424, mean_q: -42.828050, mean_eps: 0.897895\n",
            "Track generation: 1267..1595 -> 328-tiles track\n",
            " 1162/3000: episode: 65, duration: 5.236s, episode steps:  18, steps per second:   3, episode reward: -93.068, mean reward: -5.170 [-100.200,  5.816], mean action: 5.833 [0.000, 13.000],  loss: 81.683499, mae: 42.851861, mean_q: -42.717956, mean_eps: 0.896275\n",
            "Track generation: 1232..1553 -> 321-tiles track\n",
            " 1180/3000: episode: 66, duration: 5.049s, episode steps:  18, steps per second:   4, episode reward: -95.825, mean reward: -5.324 [-100.100,  5.950], mean action: 6.833 [2.000, 15.000],  loss: 119.207455, mae: 43.489252, mean_q: -42.795416, mean_eps: 0.894655\n",
            "Track generation: 1326..1662 -> 336-tiles track\n",
            " 1198/3000: episode: 67, duration: 5.065s, episode steps:  18, steps per second:   4, episode reward: -93.360, mean reward: -5.187 [-100.200,  5.670], mean action: 7.111 [0.000, 15.000],  loss: 74.160124, mae: 43.721012, mean_q: -43.271294, mean_eps: 0.893035\n",
            "Track generation: 1135..1423 -> 288-tiles track\n",
            " 1216/3000: episode: 68, duration: 5.315s, episode steps:  18, steps per second:   3, episode reward: -94.747, mean reward: -5.264 [-100.100,  6.669], mean action: 6.167 [0.000, 15.000],  loss: 86.206481, mae: 43.075344, mean_q: -42.867142, mean_eps: 0.891415\n",
            "Track generation: 1087..1367 -> 280-tiles track\n",
            " 1234/3000: episode: 69, duration: 5.017s, episode steps:  18, steps per second:   4, episode reward: -94.447, mean reward: -5.247 [-100.100,  6.868], mean action: 8.889 [1.000, 15.000],  loss: 61.526623, mae: 42.461984, mean_q: -42.852223, mean_eps: 0.889795\n",
            "Track generation: 1049..1317 -> 268-tiles track\n",
            "retry to generate track (normal if there are not manyinstances of this message)\n",
            "Track generation: 1051..1318 -> 267-tiles track\n",
            " 1251/3000: episode: 70, duration: 4.775s, episode steps:  17, steps per second:   4, episode reward: -97.581, mean reward: -5.740 [-100.300,  7.219], mean action: 9.118 [1.000, 15.000],  loss: 78.822203, mae: 41.873629, mean_q: -42.625522, mean_eps: 0.888220\n",
            "Track generation: 1321..1656 -> 335-tiles track\n",
            " 1269/3000: episode: 71, duration: 5.065s, episode steps:  18, steps per second:   4, episode reward: -96.218, mean reward: -5.345 [-100.100,  5.688], mean action: 6.722 [0.000, 15.000],  loss: 89.733130, mae: 42.475979, mean_q: -43.472043, mean_eps: 0.886645\n",
            "Track generation: 1312..1644 -> 332-tiles track\n",
            " 1287/3000: episode: 72, duration: 5.069s, episode steps:  18, steps per second:   4, episode reward: -96.137, mean reward: -5.341 [-100.100,  5.742], mean action: 8.500 [0.000, 15.000],  loss: 94.872557, mae: 42.576708, mean_q: -43.132680, mean_eps: 0.885025\n",
            "Track generation: 1196..1499 -> 303-tiles track\n",
            " 1305/3000: episode: 73, duration: 5.307s, episode steps:  18, steps per second:   3, episode reward: -95.266, mean reward: -5.293 [-100.100,  6.323], mean action: 8.444 [0.000, 15.000],  loss: 108.865849, mae: 43.128169, mean_q: -43.430134, mean_eps: 0.883405\n",
            "Track generation: 1014..1273 -> 259-tiles track\n",
            "retry to generate track (normal if there are not manyinstances of this message)\n",
            "Track generation: 1183..1483 -> 300-tiles track\n",
            " 1323/3000: episode: 74, duration: 5.077s, episode steps:  18, steps per second:   4, episode reward: -95.167, mean reward: -5.287 [-100.100,  6.389], mean action: 6.944 [0.000, 15.000],  loss: 99.532615, mae: 43.807941, mean_q: -43.885027, mean_eps: 0.881785\n",
            "Track generation: 1356..1699 -> 343-tiles track\n",
            " 1341/3000: episode: 75, duration: 5.076s, episode steps:  18, steps per second:   4, episode reward: -96.428, mean reward: -5.357 [-100.100,  5.548], mean action: 8.000 [1.000, 14.000],  loss: 90.674497, mae: 44.093503, mean_q: -44.749850, mean_eps: 0.880165\n",
            "Track generation: 1265..1588 -> 323-tiles track\n",
            "retry to generate track (normal if there are not manyinstances of this message)\n",
            "Track generation: 1203..1508 -> 305-tiles track\n",
            " 1359/3000: episode: 76, duration: 5.068s, episode steps:  18, steps per second:   4, episode reward: -95.332, mean reward: -5.296 [-100.100,  6.279], mean action: 8.444 [0.000, 15.000],  loss: 87.008610, mae: 43.678477, mean_q: -43.371917, mean_eps: 0.878545\n",
            "Track generation: 1135..1422 -> 287-tiles track\n",
            " 1377/3000: episode: 77, duration: 5.044s, episode steps:  18, steps per second:   4, episode reward: -94.710, mean reward: -5.262 [-100.100,  6.693], mean action: 9.389 [0.000, 15.000],  loss: 79.843014, mae: 43.208135, mean_q: -42.676997, mean_eps: 0.876925\n",
            "Track generation: 1089..1374 -> 285-tiles track\n",
            " 1395/3000: episode: 78, duration: 5.082s, episode steps:  18, steps per second:   4, episode reward: -94.637, mean reward: -5.258 [-100.100,  6.742], mean action: 7.611 [2.000, 15.000],  loss: 124.579863, mae: 43.643676, mean_q: -43.623713, mean_eps: 0.875305\n",
            "Track generation: 1070..1342 -> 272-tiles track\n",
            " 1413/3000: episode: 79, duration: 5.317s, episode steps:  18, steps per second:   3, episode reward: -90.540, mean reward: -5.030 [-100.200,  7.080], mean action: 6.389 [0.000, 14.000],  loss: 86.010933, mae: 44.259624, mean_q: -44.605858, mean_eps: 0.873685\n",
            "Track generation: 1384..1734 -> 350-tiles track\n",
            " 1431/3000: episode: 80, duration: 5.099s, episode steps:  18, steps per second:   4, episode reward: -96.604, mean reward: -5.367 [-100.100,  5.431], mean action: 7.278 [0.000, 14.000],  loss: 86.188633, mae: 44.293941, mean_q: -44.884794, mean_eps: 0.872065\n",
            "Track generation: 1294..1620 -> 326-tiles track\n",
            " 1449/3000: episode: 81, duration: 5.034s, episode steps:  18, steps per second:   4, episode reward: -95.969, mean reward: -5.332 [-100.100,  5.854], mean action: 8.444 [0.000, 15.000],  loss: 88.203735, mae: 44.036756, mean_q: -44.469424, mean_eps: 0.870445\n",
            "Track generation: 999..1253 -> 254-tiles track\n",
            " 1466/3000: episode: 82, duration: 4.757s, episode steps:  17, steps per second:   4, episode reward: -97.195, mean reward: -5.717 [-100.300,  7.605], mean action: 11.176 [1.000, 15.000],  loss: 83.884043, mae: 44.145837, mean_q: -45.232993, mean_eps: 0.868870\n",
            "Track generation: 1111..1393 -> 282-tiles track\n",
            " 1484/3000: episode: 83, duration: 5.036s, episode steps:  18, steps per second:   4, episode reward: -94.524, mean reward: -5.251 [-100.100,  6.817], mean action: 9.111 [0.000, 15.000],  loss: 79.057686, mae: 44.300126, mean_q: -45.333404, mean_eps: 0.867295\n",
            "Track generation: 1252..1569 -> 317-tiles track\n",
            " 1502/3000: episode: 84, duration: 5.358s, episode steps:  18, steps per second:   3, episode reward: -95.706, mean reward: -5.317 [-100.100,  6.029], mean action: 7.833 [0.000, 15.000],  loss: 109.451311, mae: 44.266976, mean_q: -44.748685, mean_eps: 0.865675\n",
            "Track generation: 1143..1433 -> 290-tiles track\n",
            " 1520/3000: episode: 85, duration: 5.084s, episode steps:  18, steps per second:   4, episode reward: -94.819, mean reward: -5.268 [-100.100,  6.620], mean action: 8.222 [2.000, 15.000],  loss: 105.981579, mae: 45.051167, mean_q: -44.939713, mean_eps: 0.864055\n",
            "Track generation: 1142..1472 -> 330-tiles track\n",
            "retry to generate track (normal if there are not manyinstances of this message)\n",
            "Track generation: 1103..1393 -> 290-tiles track\n",
            " 1538/3000: episode: 86, duration: 5.118s, episode steps:  18, steps per second:   4, episode reward: -94.819, mean reward: -5.268 [-100.100,  6.620], mean action: 6.889 [0.000, 15.000],  loss: 91.918825, mae: 45.301809, mean_q: -45.114286, mean_eps: 0.862435\n",
            "Track generation: 1137..1425 -> 288-tiles track\n",
            " 1556/3000: episode: 87, duration: 5.123s, episode steps:  18, steps per second:   4, episode reward: -94.747, mean reward: -5.264 [-100.100,  6.669], mean action: 6.389 [0.000, 15.000],  loss: 68.921386, mae: 44.866078, mean_q: -44.234598, mean_eps: 0.860815\n",
            "Track generation: 1044..1309 -> 265-tiles track\n",
            " 1574/3000: episode: 88, duration: 5.122s, episode steps:  18, steps per second:   4, episode reward: -93.836, mean reward: -5.213 [-100.100,  7.276], mean action: 7.167 [0.000, 15.000],  loss: 89.475282, mae: 44.502680, mean_q: -44.403785, mean_eps: 0.859195\n",
            "Track generation: 899..1136 -> 237-tiles track\n",
            " 1592/3000: episode: 89, duration: 5.100s, episode steps:  18, steps per second:   4, episode reward: -92.488, mean reward: -5.138 [-100.100,  8.175], mean action: 8.000 [0.000, 15.000],  loss: 83.554378, mae: 44.927919, mean_q: -45.095400, mean_eps: 0.857575\n",
            "Track generation: 1195..1498 -> 303-tiles track\n",
            " 1610/3000: episode: 90, duration: 5.422s, episode steps:  18, steps per second:   3, episode reward: -95.266, mean reward: -5.293 [-100.100,  6.323], mean action: 7.833 [0.000, 15.000],  loss: 85.559634, mae: 44.875344, mean_q: -45.125593, mean_eps: 0.855955\n",
            "Track generation: 1131..1418 -> 287-tiles track\n",
            " 1628/3000: episode: 91, duration: 5.133s, episode steps:  18, steps per second:   4, episode reward: -94.710, mean reward: -5.262 [-100.100,  6.693], mean action: 6.667 [0.000, 14.000],  loss: 73.017341, mae: 44.651521, mean_q: -45.656535, mean_eps: 0.854335\n",
            "Track generation: 987..1240 -> 253-tiles track\n",
            " 1646/3000: episode: 92, duration: 5.091s, episode steps:  18, steps per second:   4, episode reward: -93.295, mean reward: -5.183 [-100.100,  7.637], mean action: 8.444 [0.000, 15.000],  loss: 100.309463, mae: 44.919430, mean_q: -45.869786, mean_eps: 0.852715\n",
            "Track generation: 1211..1518 -> 307-tiles track\n",
            " 1664/3000: episode: 93, duration: 5.054s, episode steps:  18, steps per second:   4, episode reward: -95.396, mean reward: -5.300 [-100.100,  6.236], mean action: 7.500 [1.000, 13.000],  loss: 72.478226, mae: 45.367487, mean_q: -45.849031, mean_eps: 0.851095\n",
            "Track generation: 1059..1328 -> 269-tiles track\n",
            " 1682/3000: episode: 94, duration: 5.059s, episode steps:  18, steps per second:   4, episode reward: -94.006, mean reward: -5.223 [-100.100,  7.163], mean action: 7.833 [0.000, 15.000],  loss: 105.730362, mae: 44.797434, mean_q: -45.600167, mean_eps: 0.849475\n",
            "Track generation: 1165..1460 -> 295-tiles track\n",
            " 1700/3000: episode: 95, duration: 5.392s, episode steps:  18, steps per second:   3, episode reward: -94.996, mean reward: -5.278 [-100.100,  6.503], mean action: 8.167 [0.000, 15.000],  loss: 76.763815, mae: 45.080707, mean_q: -46.019956, mean_eps: 0.847855\n",
            "Track generation: 1143..1433 -> 290-tiles track\n",
            " 1718/3000: episode: 96, duration: 5.377s, episode steps:  18, steps per second:   3, episode reward: -94.819, mean reward: -5.268 [-100.100,  6.620], mean action: 7.444 [1.000, 15.000],  loss: 70.994706, mae: 44.895719, mean_q: -45.598978, mean_eps: 0.846235\n",
            "Track generation: 1112..1394 -> 282-tiles track\n",
            " 1736/3000: episode: 97, duration: 5.098s, episode steps:  18, steps per second:   4, episode reward: -91.065, mean reward: -5.059 [-100.200,  6.817], mean action: 6.944 [1.000, 15.000],  loss: 84.853464, mae: 44.593885, mean_q: -44.840757, mean_eps: 0.844615\n",
            "Track generation: 1231..1543 -> 312-tiles track\n",
            " 1753/3000: episode: 98, duration: 4.822s, episode steps:  17, steps per second:   4, episode reward: -98.669, mean reward: -5.804 [-100.300,  6.131], mean action: 9.235 [0.000, 14.000],  loss: 106.220575, mae: 45.468361, mean_q: -45.622676, mean_eps: 0.843040\n",
            "Track generation: 1193..1495 -> 302-tiles track\n",
            " 1771/3000: episode: 99, duration: 5.111s, episode steps:  18, steps per second:   4, episode reward: -95.233, mean reward: -5.291 [-100.100,  6.345], mean action: 7.667 [0.000, 14.000],  loss: 57.119311, mae: 45.816513, mean_q: -46.167281, mean_eps: 0.841465\n",
            "Track generation: 1214..1522 -> 308-tiles track\n",
            " 1789/3000: episode: 100, duration: 5.072s, episode steps:  18, steps per second:   4, episode reward: -95.428, mean reward: -5.302 [-100.100,  6.215], mean action: 6.722 [0.000, 15.000],  loss: 101.330252, mae: 44.988998, mean_q: -45.623890, mean_eps: 0.839845\n",
            "Track generation: 1000..1254 -> 254-tiles track\n",
            " 1807/3000: episode: 101, duration: 5.372s, episode steps:  18, steps per second:   3, episode reward: -93.342, mean reward: -5.186 [-100.100,  7.605], mean action: 7.889 [0.000, 15.000],  loss: 84.271990, mae: 45.462524, mean_q: -45.708178, mean_eps: 0.838225\n",
            "Track generation: 1210..1516 -> 306-tiles track\n",
            " 1824/3000: episode: 102, duration: 4.843s, episode steps:  17, steps per second:   4, episode reward: -98.543, mean reward: -5.797 [-100.300,  6.257], mean action: 10.765 [4.000, 15.000],  loss: 64.171072, mae: 45.174636, mean_q: -45.280277, mean_eps: 0.836650\n",
            "Track generation: 1135..1432 -> 297-tiles track\n",
            " 1842/3000: episode: 103, duration: 5.098s, episode steps:  18, steps per second:   4, episode reward: -95.065, mean reward: -5.281 [-100.100,  6.457], mean action: 7.444 [0.000, 13.000],  loss: 64.376765, mae: 44.935491, mean_q: -45.298758, mean_eps: 0.835075\n",
            "Track generation: 1074..1349 -> 275-tiles track\n",
            "retry to generate track (normal if there are not manyinstances of this message)\n",
            "Track generation: 1199..1503 -> 304-tiles track\n",
            " 1860/3000: episode: 104, duration: 5.102s, episode steps:  18, steps per second:   4, episode reward: -95.299, mean reward: -5.294 [-100.100,  6.301], mean action: 7.889 [0.000, 14.000],  loss: 83.322438, mae: 45.083182, mean_q: -45.664912, mean_eps: 0.833455\n",
            "Track generation: 1136..1424 -> 288-tiles track\n",
            " 1878/3000: episode: 105, duration: 5.122s, episode steps:  18, steps per second:   4, episode reward: -94.747, mean reward: -5.264 [-100.100,  6.669], mean action: 7.500 [1.000, 15.000],  loss: 84.947683, mae: 45.516958, mean_q: -46.034612, mean_eps: 0.831835\n",
            "Track generation: 1143..1433 -> 290-tiles track\n",
            " 1896/3000: episode: 106, duration: 5.098s, episode steps:  18, steps per second:   4, episode reward: -94.819, mean reward: -5.268 [-100.100,  6.620], mean action: 7.278 [1.000, 13.000],  loss: 80.899685, mae: 45.735888, mean_q: -45.998455, mean_eps: 0.830215\n",
            "Track generation: 1130..1422 -> 292-tiles track\n",
            " 1913/3000: episode: 107, duration: 5.081s, episode steps:  17, steps per second:   3, episode reward: -98.227, mean reward: -5.778 [-100.300,  6.573], mean action: 9.176 [0.000, 14.000],  loss: 86.786244, mae: 45.563624, mean_q: -45.393778, mean_eps: 0.828640\n",
            "Track generation: 1330..1668 -> 338-tiles track\n",
            " 1930/3000: episode: 108, duration: 4.830s, episode steps:  17, steps per second:   4, episode reward: -99.165, mean reward: -5.833 [-100.300,  5.635], mean action: 10.471 [1.000, 15.000],  loss: 98.459283, mae: 45.432777, mean_q: -44.890565, mean_eps: 0.827110\n",
            "Track generation: 1191..1493 -> 302-tiles track\n",
            " 1947/3000: episode: 109, duration: 4.798s, episode steps:  17, steps per second:   4, episode reward: -98.455, mean reward: -5.791 [-100.300,  6.345], mean action: 9.882 [1.000, 15.000],  loss: 98.380270, mae: 45.843969, mean_q: -45.499702, mean_eps: 0.825580\n",
            "Track generation: 1139..1428 -> 289-tiles track\n",
            " 1965/3000: episode: 110, duration: 5.040s, episode steps:  18, steps per second:   4, episode reward: -94.783, mean reward: -5.266 [-100.100,  6.644], mean action: 8.389 [0.000, 15.000],  loss: 89.351615, mae: 45.987011, mean_q: -47.035793, mean_eps: 0.824005\n",
            "Track generation: 1097..1375 -> 278-tiles track\n",
            " 1983/3000: episode: 111, duration: 5.073s, episode steps:  18, steps per second:   4, episode reward: -94.370, mean reward: -5.243 [-100.100,  6.920], mean action: 10.167 [1.000, 15.000],  loss: 58.295970, mae: 45.395333, mean_q: -45.927795, mean_eps: 0.822385\n",
            "Track generation: 1189..1490 -> 301-tiles track\n",
            " 2001/3000: episode: 112, duration: 5.354s, episode steps:  18, steps per second:   3, episode reward: -95.200, mean reward: -5.289 [-100.100,  6.367], mean action: 7.889 [0.000, 15.000],  loss: 65.760606, mae: 44.461245, mean_q: -44.613994, mean_eps: 0.820765\n",
            "Track generation: 1332..1670 -> 338-tiles track\n",
            " 2019/3000: episode: 113, duration: 5.142s, episode steps:  18, steps per second:   4, episode reward: -96.298, mean reward: -5.350 [-100.100,  5.635], mean action: 8.389 [0.000, 15.000],  loss: 91.637077, mae: 44.454420, mean_q: -45.082620, mean_eps: 0.819145\n",
            "Track generation: 1430..1798 -> 368-tiles track\n",
            " 2037/3000: episode: 114, duration: 5.136s, episode steps:  18, steps per second:   4, episode reward: -97.026, mean reward: -5.390 [-100.100,  5.150], mean action: 7.500 [0.000, 15.000],  loss: 91.264279, mae: 44.719442, mean_q: -45.296544, mean_eps: 0.817525\n",
            "Track generation: 1160..1454 -> 294-tiles track\n",
            " 2055/3000: episode: 115, duration: 5.131s, episode steps:  18, steps per second:   4, episode reward: -94.961, mean reward: -5.276 [-100.100,  6.526], mean action: 6.278 [0.000, 15.000],  loss: 83.157654, mae: 44.820641, mean_q: -44.863511, mean_eps: 0.815905\n",
            "Track generation: 1100..1379 -> 279-tiles track\n",
            " 2073/3000: episode: 116, duration: 5.070s, episode steps:  18, steps per second:   4, episode reward: -94.409, mean reward: -5.245 [-100.100,  6.894], mean action: 7.944 [0.000, 14.000],  loss: 76.472729, mae: 44.350710, mean_q: -44.260600, mean_eps: 0.814285\n",
            "Track generation: 1084..1363 -> 279-tiles track\n",
            " 2090/3000: episode: 117, duration: 4.855s, episode steps:  17, steps per second:   4, episode reward: -97.906, mean reward: -5.759 [-100.300,  6.894], mean action: 9.176 [1.000, 14.000],  loss: 81.319869, mae: 44.196019, mean_q: -44.158098, mean_eps: 0.812710\n",
            "Track generation: 1164..1459 -> 295-tiles track\n",
            " 2108/3000: episode: 118, duration: 5.391s, episode steps:  18, steps per second:   3, episode reward: -94.996, mean reward: -5.278 [-100.100,  6.503], mean action: 10.111 [2.000, 15.000],  loss: 93.356515, mae: 44.595569, mean_q: -44.365837, mean_eps: 0.811135\n",
            "Track generation: 979..1228 -> 249-tiles track\n",
            " 2126/3000: episode: 119, duration: 5.088s, episode steps:  18, steps per second:   4, episode reward: -93.103, mean reward: -5.172 [-100.100,  7.765], mean action: 8.056 [1.000, 15.000],  loss: 73.324538, mae: 44.359211, mean_q: -44.477857, mean_eps: 0.809515\n",
            "Track generation: 1172..1469 -> 297-tiles track\n",
            " 2144/3000: episode: 120, duration: 5.080s, episode steps:  18, steps per second:   4, episode reward: -95.065, mean reward: -5.281 [-100.100,  6.457], mean action: 7.889 [0.000, 15.000],  loss: 85.700780, mae: 43.758259, mean_q: -43.865183, mean_eps: 0.807895\n",
            "Track generation: 1092..1369 -> 277-tiles track\n",
            " 2161/3000: episode: 121, duration: 4.775s, episode steps:  17, steps per second:   4, episode reward: -97.854, mean reward: -5.756 [-100.300,  6.946], mean action: 9.176 [0.000, 15.000],  loss: 71.274446, mae: 43.925189, mean_q: -44.728180, mean_eps: 0.806320\n",
            "Track generation: 1105..1389 -> 284-tiles track\n",
            " 2179/3000: episode: 122, duration: 4.962s, episode steps:  18, steps per second:   4, episode reward: -94.599, mean reward: -5.256 [-100.100,  6.767], mean action: 6.667 [0.000, 13.000],  loss: 88.529762, mae: 44.020496, mean_q: -44.987425, mean_eps: 0.804745\n",
            "Track generation: 1194..1497 -> 303-tiles track\n",
            " 2196/3000: episode: 123, duration: 4.746s, episode steps:  17, steps per second:   4, episode reward: -98.477, mean reward: -5.793 [-100.300,  6.323], mean action: 9.412 [0.000, 15.000],  loss: 83.215501, mae: 44.003366, mean_q: -45.208551, mean_eps: 0.803170\n",
            "Track generation: 1013..1275 -> 262-tiles track\n",
            " 2214/3000: episode: 124, duration: 5.295s, episode steps:  18, steps per second:   3, episode reward: -93.706, mean reward: -5.206 [-100.100,  7.363], mean action: 9.389 [2.000, 15.000],  loss: 89.100799, mae: 43.978785, mean_q: -44.774473, mean_eps: 0.801595\n",
            "Track generation: 1163..1458 -> 295-tiles track\n",
            " 2232/3000: episode: 125, duration: 4.993s, episode steps:  18, steps per second:   4, episode reward: -94.996, mean reward: -5.278 [-100.100,  6.503], mean action: 7.833 [1.000, 14.000],  loss: 108.947595, mae: 44.747296, mean_q: -45.328984, mean_eps: 0.799975\n",
            "Track generation: 1197..1509 -> 312-tiles track\n",
            " 2250/3000: episode: 126, duration: 5.028s, episode steps:  18, steps per second:   4, episode reward: -95.554, mean reward: -5.309 [-100.100,  6.131], mean action: 8.278 [1.000, 14.000],  loss: 99.554208, mae: 45.279584, mean_q: -45.530439, mean_eps: 0.798355\n",
            "Track generation: 1123..1408 -> 285-tiles track\n",
            " 2267/3000: episode: 127, duration: 4.768s, episode steps:  17, steps per second:   4, episode reward: -98.058, mean reward: -5.768 [-100.300,  6.742], mean action: 8.118 [2.000, 13.000],  loss: 94.933902, mae: 45.098308, mean_q: -45.380907, mean_eps: 0.796780\n",
            "Track generation: 1171..1468 -> 297-tiles track\n",
            " 2285/3000: episode: 128, duration: 5.016s, episode steps:  18, steps per second:   4, episode reward: -95.065, mean reward: -5.281 [-100.100,  6.457], mean action: 8.111 [1.000, 15.000],  loss: 82.332077, mae: 44.788491, mean_q: -45.108259, mean_eps: 0.795205\n",
            "Track generation: 1202..1507 -> 305-tiles track\n",
            " 2302/3000: episode: 129, duration: 5.009s, episode steps:  17, steps per second:   3, episode reward: -98.521, mean reward: -5.795 [-100.300,  6.279], mean action: 9.235 [1.000, 12.000],  loss: 68.831410, mae: 44.790252, mean_q: -44.526655, mean_eps: 0.793630\n",
            "Track generation: 1008..1264 -> 256-tiles track\n",
            " 2320/3000: episode: 130, duration: 4.967s, episode steps:  18, steps per second:   4, episode reward: -93.435, mean reward: -5.191 [-100.100,  7.543], mean action: 7.444 [0.000, 15.000],  loss: 92.383154, mae: 44.104727, mean_q: -43.900674, mean_eps: 0.792055\n",
            "Track generation: 1312..1644 -> 332-tiles track\n",
            " 2338/3000: episode: 131, duration: 5.015s, episode steps:  18, steps per second:   4, episode reward: -90.294, mean reward: -5.016 [-100.300,  5.742], mean action: 4.889 [0.000, 11.000],  loss: 92.581825, mae: 43.710095, mean_q: -44.009057, mean_eps: 0.790435\n",
            "Track generation: 1259..1578 -> 319-tiles track\n",
            " 2356/3000: episode: 132, duration: 4.982s, episode steps:  18, steps per second:   4, episode reward: -92.721, mean reward: -5.151 [-100.200,  5.989], mean action: 5.722 [0.000, 15.000],  loss: 101.227741, mae: 44.313309, mean_q: -44.875282, mean_eps: 0.788815\n",
            "Track generation: 1120..1404 -> 284-tiles track\n",
            " 2374/3000: episode: 133, duration: 4.951s, episode steps:  18, steps per second:   4, episode reward: -94.599, mean reward: -5.256 [-100.100,  6.767], mean action: 8.111 [1.000, 15.000],  loss: 91.914193, mae: 44.590628, mean_q: -44.794703, mean_eps: 0.787195\n",
            "Track generation: 1139..1384 -> 245-tiles track\n",
            "retry to generate track (normal if there are not manyinstances of this message)\n",
            "Track generation: 1090..1369 -> 279-tiles track\n",
            "retry to generate track (normal if there are not manyinstances of this message)\n",
            "Track generation: 1209..1523 -> 314-tiles track\n",
            " 2392/3000: episode: 134, duration: 4.948s, episode steps:  18, steps per second:   4, episode reward: -92.520, mean reward: -5.140 [-100.200,  6.090], mean action: 6.056 [0.000, 15.000],  loss: 74.695376, mae: 44.101909, mean_q: -45.043115, mean_eps: 0.785575\n",
            "Track generation: 1068..1339 -> 271-tiles track\n",
            " 2410/3000: episode: 135, duration: 5.201s, episode steps:  18, steps per second:   3, episode reward: -94.089, mean reward: -5.227 [-100.100,  7.107], mean action: 7.889 [0.000, 15.000],  loss: 64.799980, mae: 43.570970, mean_q: -44.890431, mean_eps: 0.783955\n",
            "Track generation: 1043..1308 -> 265-tiles track\n",
            " 2428/3000: episode: 136, duration: 4.956s, episode steps:  18, steps per second:   4, episode reward: -90.148, mean reward: -5.008 [-100.200,  7.276], mean action: 6.333 [0.000, 15.000],  loss: 77.342741, mae: 44.039118, mean_q: -45.431065, mean_eps: 0.782335\n",
            "Track generation: 1069..1346 -> 277-tiles track\n",
            " 2446/3000: episode: 137, duration: 4.972s, episode steps:  18, steps per second:   4, episode reward: -94.330, mean reward: -5.241 [-100.100,  6.946], mean action: 8.278 [0.000, 15.000],  loss: 91.065447, mae: 44.672932, mean_q: -46.071822, mean_eps: 0.780715\n",
            "Track generation: 1156..1449 -> 293-tiles track\n",
            " 2464/3000: episode: 138, duration: 4.928s, episode steps:  18, steps per second:   4, episode reward: -94.926, mean reward: -5.274 [-100.100,  6.549], mean action: 8.500 [2.000, 14.000],  loss: 94.090036, mae: 45.044570, mean_q: -45.865353, mean_eps: 0.779095\n",
            "Track generation: 1080..1354 -> 274-tiles track\n",
            " 2482/3000: episode: 139, duration: 4.933s, episode steps:  18, steps per second:   4, episode reward: -94.211, mean reward: -5.234 [-100.100,  7.026], mean action: 7.667 [2.000, 14.000],  loss: 102.269895, mae: 45.049983, mean_q: -45.747147, mean_eps: 0.777475\n",
            "Track generation: 1280..1604 -> 324-tiles track\n",
            " 2500/3000: episode: 140, duration: 5.223s, episode steps:  18, steps per second:   3, episode reward: -95.912, mean reward: -5.328 [-100.100,  5.892], mean action: 7.944 [0.000, 14.000],  loss: 95.320913, mae: 45.099283, mean_q: -46.115110, mean_eps: 0.775855\n",
            "Track generation: 1287..1613 -> 326-tiles track\n",
            " 2518/3000: episode: 141, duration: 5.220s, episode steps:  18, steps per second:   3, episode reward: -95.969, mean reward: -5.332 [-100.100,  5.854], mean action: 8.000 [1.000, 15.000],  loss: 74.533582, mae: 45.198302, mean_q: -45.698638, mean_eps: 0.774235\n",
            "Track generation: 1016..1276 -> 260-tiles track\n",
            "retry to generate track (normal if there are not manyinstances of this message)\n",
            "Track generation: 1185..1494 -> 309-tiles track\n",
            " 2535/3000: episode: 142, duration: 4.681s, episode steps:  17, steps per second:   4, episode reward: -98.606, mean reward: -5.800 [-100.300,  6.194], mean action: 9.294 [1.000, 15.000],  loss: 65.851852, mae: 45.049339, mean_q: -45.375651, mean_eps: 0.772660\n",
            "Track generation: 1096..1374 -> 278-tiles track\n",
            " 2553/3000: episode: 143, duration: 4.943s, episode steps:  18, steps per second:   4, episode reward: -90.860, mean reward: -5.048 [-100.200,  6.920], mean action: 6.611 [0.000, 15.000],  loss: 94.507858, mae: 44.925754, mean_q: -46.385317, mean_eps: 0.771085\n",
            "Track generation: 1213..1520 -> 307-tiles track\n",
            " 2571/3000: episode: 144, duration: 4.923s, episode steps:  18, steps per second:   4, episode reward: -95.396, mean reward: -5.300 [-100.100,  6.236], mean action: 9.667 [0.000, 15.000],  loss: 83.478468, mae: 45.537173, mean_q: -47.309808, mean_eps: 0.769465\n",
            "Track generation: 1169..1465 -> 296-tiles track\n",
            " 2589/3000: episode: 145, duration: 4.949s, episode steps:  18, steps per second:   4, episode reward: -88.451, mean reward: -4.914 [-100.300,  6.480], mean action: 5.056 [0.000, 15.000],  loss: 57.705288, mae: 45.398548, mean_q: -46.763364, mean_eps: 0.767845\n",
            "Track generation: 973..1222 -> 249-tiles track\n",
            " 2606/3000: episode: 146, duration: 4.941s, episode steps:  17, steps per second:   3, episode reward: -97.035, mean reward: -5.708 [-100.300,  7.765], mean action: 10.706 [2.000, 15.000],  loss: 88.823981, mae: 45.050411, mean_q: -45.803698, mean_eps: 0.766270\n",
            "Track generation: 1212..1521 -> 309-tiles track\n",
            " 2624/3000: episode: 147, duration: 4.936s, episode steps:  18, steps per second:   4, episode reward: -95.460, mean reward: -5.303 [-100.100,  6.194], mean action: 8.444 [0.000, 15.000],  loss: 65.689616, mae: 45.209379, mean_q: -46.400158, mean_eps: 0.764695\n",
            "Track generation: 1215..1525 -> 310-tiles track\n",
            "retry to generate track (normal if there are not manyinstances of this message)\n",
            "Track generation: 1169..1465 -> 296-tiles track\n",
            " 2641/3000: episode: 148, duration: 4.663s, episode steps:  17, steps per second:   4, episode reward: -98.320, mean reward: -5.784 [-100.300,  6.480], mean action: 7.824 [1.000, 13.000],  loss: 73.924663, mae: 44.606164, mean_q: -45.361621, mean_eps: 0.763120\n",
            "Track generation: 1409..1766 -> 357-tiles track\n",
            " 2659/3000: episode: 149, duration: 4.930s, episode steps:  18, steps per second:   4, episode reward: -94.064, mean reward: -5.226 [-100.200,  5.318], mean action: 7.389 [1.000, 15.000],  loss: 71.646598, mae: 44.917851, mean_q: -45.604734, mean_eps: 0.761545\n",
            "Track generation: 1305..1643 -> 338-tiles track\n",
            " 2676/3000: episode: 150, duration: 4.670s, episode steps:  17, steps per second:   4, episode reward: -99.165, mean reward: -5.833 [-100.300,  5.635], mean action: 8.059 [2.000, 15.000],  loss: 108.222764, mae: 45.548833, mean_q: -45.775758, mean_eps: 0.759970\n",
            "Track generation: 1257..1575 -> 318-tiles track\n",
            " 2694/3000: episode: 151, duration: 4.910s, episode steps:  18, steps per second:   4, episode reward: -95.736, mean reward: -5.319 [-100.100,  6.009], mean action: 9.833 [0.000, 15.000],  loss: 96.498152, mae: 45.827028, mean_q: -46.409222, mean_eps: 0.758395\n",
            "Track generation: 1061..1330 -> 269-tiles track\n",
            " 2712/3000: episode: 152, duration: 5.162s, episode steps:  18, steps per second:   3, episode reward: -94.006, mean reward: -5.223 [-100.100,  7.163], mean action: 10.222 [0.000, 15.000],  loss: 87.176772, mae: 45.518593, mean_q: -45.595321, mean_eps: 0.756775\n",
            "Track generation: 1143..1433 -> 290-tiles track\n",
            " 2730/3000: episode: 153, duration: 4.902s, episode steps:  18, steps per second:   4, episode reward: -94.819, mean reward: -5.268 [-100.100,  6.620], mean action: 8.667 [0.000, 15.000],  loss: 101.057318, mae: 45.525553, mean_q: -45.722178, mean_eps: 0.755155\n",
            "Track generation: 1090..1367 -> 277-tiles track\n",
            " 2748/3000: episode: 154, duration: 4.920s, episode steps:  18, steps per second:   4, episode reward: -94.330, mean reward: -5.241 [-100.100,  6.946], mean action: 11.167 [1.000, 15.000],  loss: 73.377499, mae: 45.600485, mean_q: -46.103090, mean_eps: 0.753535\n",
            "Track generation: 1136..1428 -> 292-tiles track\n",
            "retry to generate track (normal if there are not manyinstances of this message)\n",
            "Track generation: 1180..1479 -> 299-tiles track\n",
            " 2765/3000: episode: 155, duration: 4.681s, episode steps:  17, steps per second:   4, episode reward: -98.389, mean reward: -5.788 [-100.300,  6.411], mean action: 10.176 [4.000, 15.000],  loss: 86.601680, mae: 45.327096, mean_q: -46.573295, mean_eps: 0.751960\n",
            "Track generation: 1165..1460 -> 295-tiles track\n",
            " 2783/3000: episode: 156, duration: 4.937s, episode steps:  18, steps per second:   4, episode reward: -88.393, mean reward: -4.911 [-100.300,  6.503], mean action: 5.389 [0.000, 15.000],  loss: 78.860659, mae: 45.577758, mean_q: -46.405161, mean_eps: 0.750385\n",
            "Track generation: 1289..1616 -> 327-tiles track\n",
            " 2801/3000: episode: 157, duration: 5.210s, episode steps:  18, steps per second:   3, episode reward: -95.998, mean reward: -5.333 [-100.100,  5.835], mean action: 6.556 [0.000, 14.000],  loss: 75.442810, mae: 45.433114, mean_q: -45.755199, mean_eps: 0.748765\n",
            "Track generation: 1123..1416 -> 293-tiles track\n",
            " 2819/3000: episode: 158, duration: 4.917s, episode steps:  18, steps per second:   4, episode reward: -94.926, mean reward: -5.274 [-100.100,  6.549], mean action: 6.944 [2.000, 14.000],  loss: 83.354908, mae: 45.165238, mean_q: -46.069844, mean_eps: 0.747145\n",
            "Track generation: 1176..1474 -> 298-tiles track\n",
            " 2837/3000: episode: 159, duration: 4.910s, episode steps:  18, steps per second:   4, episode reward: -95.099, mean reward: -5.283 [-100.100,  6.434], mean action: 6.000 [1.000, 14.000],  loss: 69.485787, mae: 44.995659, mean_q: -46.050901, mean_eps: 0.745525\n",
            "Track generation: 1042..1312 -> 270-tiles track\n",
            " 2855/3000: episode: 160, duration: 5.028s, episode steps:  18, steps per second:   4, episode reward: -94.048, mean reward: -5.225 [-100.100,  7.135], mean action: 6.222 [0.000, 15.000],  loss: 77.450139, mae: 44.777552, mean_q: -45.522219, mean_eps: 0.743905\n",
            "Track generation: 1033..1302 -> 269-tiles track\n",
            " 2872/3000: episode: 161, duration: 4.788s, episode steps:  17, steps per second:   4, episode reward: -97.637, mean reward: -5.743 [-100.300,  7.163], mean action: 8.765 [4.000, 14.000],  loss: 84.232402, mae: 45.178987, mean_q: -46.450054, mean_eps: 0.742330\n",
            "Track generation: 1094..1371 -> 277-tiles track\n",
            " 2890/3000: episode: 162, duration: 5.040s, episode steps:  18, steps per second:   4, episode reward: -94.330, mean reward: -5.241 [-100.100,  6.946], mean action: 9.444 [0.000, 15.000],  loss: 69.749463, mae: 45.344164, mean_q: -46.685072, mean_eps: 0.740755\n",
            "Track generation: 1040..1314 -> 274-tiles track\n",
            " 2908/3000: episode: 163, duration: 5.385s, episode steps:  18, steps per second:   3, episode reward: -94.211, mean reward: -5.234 [-100.100,  7.026], mean action: 9.056 [1.000, 15.000],  loss: 79.740478, mae: 45.096892, mean_q: -46.149793, mean_eps: 0.739135\n",
            "Track generation: 1253..1570 -> 317-tiles track\n",
            " 2925/3000: episode: 164, duration: 4.847s, episode steps:  17, steps per second:   4, episode reward: -98.771, mean reward: -5.810 [-100.300,  6.029], mean action: 11.176 [2.000, 15.000],  loss: 81.162492, mae: 45.241074, mean_q: -46.321347, mean_eps: 0.737560\n",
            "Track generation: 1215..1523 -> 308-tiles track\n",
            " 2943/3000: episode: 165, duration: 5.097s, episode steps:  18, steps per second:   4, episode reward: -95.428, mean reward: -5.302 [-100.100,  6.215], mean action: 8.444 [1.000, 15.000],  loss: 67.768792, mae: 45.749165, mean_q: -46.953070, mean_eps: 0.735985\n",
            "Track generation: 1202..1506 -> 304-tiles track\n",
            " 2961/3000: episode: 166, duration: 5.121s, episode steps:  18, steps per second:   4, episode reward: -95.299, mean reward: -5.294 [-100.100,  6.301], mean action: 9.333 [0.000, 15.000],  loss: 67.328266, mae: 45.189853, mean_q: -46.560601, mean_eps: 0.734365\n",
            "Track generation: 1060..1329 -> 269-tiles track\n",
            " 2979/3000: episode: 167, duration: 5.123s, episode steps:  18, steps per second:   4, episode reward: -90.375, mean reward: -5.021 [-100.200,  7.163], mean action: 6.889 [1.000, 15.000],  loss: 79.896361, mae: 45.238038, mean_q: -46.736213, mean_eps: 0.732745\n",
            "Track generation: 1080..1354 -> 274-tiles track\n",
            " 2997/3000: episode: 168, duration: 5.101s, episode steps:  18, steps per second:   4, episode reward: -94.211, mean reward: -5.234 [-100.100,  7.026], mean action: 6.278 [0.000, 15.000],  loss: 92.522518, mae: 45.983254, mean_q: -47.657855, mean_eps: 0.731125\n",
            "Track generation: 1000..1254 -> 254-tiles track\n",
            "done, took 727.280 seconds\n",
            "Training for 3000 steps ...\n",
            "Track generation: 1116..1401 -> 285-tiles track\n",
            "retry to generate track (normal if there are not manyinstances of this message)\n",
            "Track generation: 1192..1494 -> 302-tiles track\n",
            "   18/3000: episode: 1, duration: 0.790s, episode steps:  18, steps per second:  23, episode reward: -95.233, mean reward: -5.291 [-100.100,  6.345], mean action: 8.333 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1057..1330 -> 273-tiles track\n",
            "   35/3000: episode: 2, duration: 0.475s, episode steps:  17, steps per second:  36, episode reward: -97.747, mean reward: -5.750 [-100.300,  7.053], mean action: 10.059 [2.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1020..1281 -> 261-tiles track\n",
            "retry to generate track (normal if there are not manyinstances of this message)\n",
            "Track generation: 1119..1403 -> 284-tiles track\n",
            "   53/3000: episode: 3, duration: 0.498s, episode steps:  18, steps per second:  36, episode reward: -94.599, mean reward: -5.256 [-100.100,  6.767], mean action: 5.722 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1196..1499 -> 303-tiles track\n",
            "   71/3000: episode: 4, duration: 0.498s, episode steps:  18, steps per second:  36, episode reward: -95.266, mean reward: -5.293 [-100.100,  6.323], mean action: 6.611 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1079..1353 -> 274-tiles track\n",
            "   89/3000: episode: 5, duration: 0.490s, episode steps:  18, steps per second:  37, episode reward: -90.648, mean reward: -5.036 [-100.200,  7.026], mean action: 5.778 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1069..1341 -> 272-tiles track\n",
            "  107/3000: episode: 6, duration: 0.786s, episode steps:  18, steps per second:  23, episode reward: -94.130, mean reward: -5.229 [-100.100,  7.080], mean action: 6.833 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1058..1328 -> 270-tiles track\n",
            "retry to generate track (normal if there are not manyinstances of this message)\n",
            "Track generation: 1288..1618 -> 330-tiles track\n",
            "  124/3000: episode: 7, duration: 0.496s, episode steps:  17, steps per second:  34, episode reward: -99.021, mean reward: -5.825 [-100.300,  5.779], mean action: 10.000 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1167..1468 -> 301-tiles track\n",
            "  142/3000: episode: 8, duration: 0.494s, episode steps:  18, steps per second:  36, episode reward: -95.200, mean reward: -5.289 [-100.100,  6.367], mean action: 7.222 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1005..1267 -> 262-tiles track\n",
            "  160/3000: episode: 9, duration: 0.489s, episode steps:  18, steps per second:  37, episode reward: -89.974, mean reward: -4.999 [-100.200,  7.363], mean action: 7.556 [1.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1088..1373 -> 285-tiles track\n",
            "  178/3000: episode: 10, duration: 0.492s, episode steps:  18, steps per second:  37, episode reward: -94.637, mean reward: -5.258 [-100.100,  6.742], mean action: 6.389 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1098..1375 -> 277-tiles track\n",
            "  196/3000: episode: 11, duration: 0.484s, episode steps:  18, steps per second:  37, episode reward: -94.330, mean reward: -5.241 [-100.100,  6.946], mean action: 7.889 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1212..1519 -> 307-tiles track\n",
            "  214/3000: episode: 12, duration: 0.780s, episode steps:  18, steps per second:  23, episode reward: -95.396, mean reward: -5.300 [-100.100,  6.236], mean action: 6.333 [1.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1059..1328 -> 269-tiles track\n",
            "  232/3000: episode: 13, duration: 0.497s, episode steps:  18, steps per second:  36, episode reward: -94.006, mean reward: -5.223 [-100.100,  7.163], mean action: 7.056 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1153..1454 -> 301-tiles track\n",
            "  250/3000: episode: 14, duration: 0.495s, episode steps:  18, steps per second:  36, episode reward: -95.200, mean reward: -5.289 [-100.100,  6.367], mean action: 6.556 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1017..1275 -> 258-tiles track\n",
            "  268/3000: episode: 15, duration: 0.499s, episode steps:  18, steps per second:  36, episode reward: -89.736, mean reward: -4.985 [-100.200,  7.482], mean action: 6.944 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1175..1473 -> 298-tiles track\n",
            "  286/3000: episode: 16, duration: 0.488s, episode steps:  18, steps per second:  37, episode reward: -95.099, mean reward: -5.283 [-100.100,  6.434], mean action: 7.222 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1000..1263 -> 263-tiles track\n",
            "  304/3000: episode: 17, duration: 0.779s, episode steps:  18, steps per second:  23, episode reward: -93.750, mean reward: -5.208 [-100.100,  7.334], mean action: 7.556 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1044..1310 -> 266-tiles track\n",
            "  322/3000: episode: 18, duration: 0.505s, episode steps:  18, steps per second:  36, episode reward: -93.879, mean reward: -5.216 [-100.100,  7.247], mean action: 7.667 [1.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1045..1311 -> 266-tiles track\n",
            "  340/3000: episode: 19, duration: 0.515s, episode steps:  18, steps per second:  35, episode reward: -93.879, mean reward: -5.216 [-100.100,  7.247], mean action: 7.389 [1.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1421..1781 -> 360-tiles track\n",
            "  358/3000: episode: 20, duration: 0.514s, episode steps:  18, steps per second:  35, episode reward: -96.843, mean reward: -5.380 [-100.100,  5.271], mean action: 6.833 [1.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1200..1504 -> 304-tiles track\n",
            "  376/3000: episode: 21, duration: 0.495s, episode steps:  18, steps per second:  36, episode reward: -95.299, mean reward: -5.294 [-100.100,  6.301], mean action: 8.167 [1.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1129..1415 -> 286-tiles track\n",
            "  394/3000: episode: 22, duration: 0.489s, episode steps:  18, steps per second:  37, episode reward: -94.674, mean reward: -5.260 [-100.100,  6.718], mean action: 6.611 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1192..1494 -> 302-tiles track\n",
            "  411/3000: episode: 23, duration: 0.776s, episode steps:  17, steps per second:  22, episode reward: -98.455, mean reward: -5.791 [-100.300,  6.345], mean action: 10.294 [2.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1176..1475 -> 299-tiles track\n",
            "  429/3000: episode: 24, duration: 0.488s, episode steps:  18, steps per second:  37, episode reward: -95.133, mean reward: -5.285 [-100.100,  6.411], mean action: 6.889 [1.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1085..1367 -> 282-tiles track\n",
            "  447/3000: episode: 25, duration: 0.494s, episode steps:  18, steps per second:  36, episode reward: -94.524, mean reward: -5.251 [-100.100,  6.817], mean action: 8.500 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1174..1471 -> 297-tiles track\n",
            "  465/3000: episode: 26, duration: 0.513s, episode steps:  18, steps per second:  35, episode reward: -95.065, mean reward: -5.281 [-100.100,  6.457], mean action: 6.333 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1112..1394 -> 282-tiles track\n",
            "  483/3000: episode: 27, duration: 0.504s, episode steps:  18, steps per second:  36, episode reward: -94.524, mean reward: -5.251 [-100.100,  6.817], mean action: 6.444 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1122..1416 -> 294-tiles track\n",
            "  501/3000: episode: 28, duration: 1.031s, episode steps:  18, steps per second:  17, episode reward: -94.961, mean reward: -5.276 [-100.100,  6.526], mean action: 8.278 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "Track generation: 1254..1572 -> 318-tiles track\n",
            "  519/3000: episode: 29, duration: 5.104s, episode steps:  18, steps per second:   4, episode reward: -95.736, mean reward: -5.319 [-100.100,  6.009], mean action: 8.500 [0.000, 15.000],  loss: 84.761781, mae: 46.597709, mean_q: -48.078819, mean_eps: 0.954145\n",
            "Track generation: 1156..1449 -> 293-tiles track\n",
            "  537/3000: episode: 30, duration: 5.177s, episode steps:  18, steps per second:   3, episode reward: -94.926, mean reward: -5.274 [-100.100,  6.549], mean action: 7.500 [0.000, 15.000],  loss: 66.761403, mae: 47.227621, mean_q: -48.382140, mean_eps: 0.952525\n",
            "Track generation: 1101..1380 -> 279-tiles track\n",
            "  555/3000: episode: 31, duration: 5.126s, episode steps:  18, steps per second:   4, episode reward: -94.409, mean reward: -5.245 [-100.100,  6.894], mean action: 6.722 [1.000, 15.000],  loss: 87.205842, mae: 46.778241, mean_q: -48.113136, mean_eps: 0.950905\n",
            "Track generation: 1150..1443 -> 293-tiles track\n",
            "retry to generate track (normal if there are not manyinstances of this message)\n",
            "Track generation: 1210..1516 -> 306-tiles track\n",
            "  573/3000: episode: 32, duration: 5.106s, episode steps:  18, steps per second:   4, episode reward: -95.364, mean reward: -5.298 [-100.100,  6.257], mean action: 7.500 [1.000, 13.000],  loss: 75.716968, mae: 47.060295, mean_q: -47.721084, mean_eps: 0.949285\n",
            "Track generation: 1217..1523 -> 306-tiles track\n",
            "  591/3000: episode: 33, duration: 5.128s, episode steps:  18, steps per second:   4, episode reward: -95.364, mean reward: -5.298 [-100.100,  6.257], mean action: 8.722 [0.000, 15.000],  loss: 43.546098, mae: 46.685251, mean_q: -47.886270, mean_eps: 0.947665\n",
            "Track generation: 1200..1504 -> 304-tiles track\n",
            "  609/3000: episode: 34, duration: 5.382s, episode steps:  18, steps per second:   3, episode reward: -95.299, mean reward: -5.294 [-100.100,  6.301], mean action: 8.500 [0.000, 13.000],  loss: 98.998653, mae: 46.533442, mean_q: -47.408365, mean_eps: 0.946045\n",
            "Track generation: 1139..1438 -> 299-tiles track\n",
            "  627/3000: episode: 35, duration: 5.124s, episode steps:  18, steps per second:   4, episode reward: -95.133, mean reward: -5.285 [-100.100,  6.411], mean action: 6.111 [0.000, 15.000],  loss: 82.261446, mae: 47.247821, mean_q: -47.960982, mean_eps: 0.944425\n",
            "Track generation: 1124..1409 -> 285-tiles track\n",
            "  645/3000: episode: 36, duration: 5.123s, episode steps:  18, steps per second:   4, episode reward: -94.637, mean reward: -5.258 [-100.100,  6.742], mean action: 7.056 [0.000, 15.000],  loss: 84.576880, mae: 47.008477, mean_q: -47.962440, mean_eps: 0.942805\n",
            "Track generation: 1249..1571 -> 322-tiles track\n",
            "  663/3000: episode: 37, duration: 5.147s, episode steps:  18, steps per second:   3, episode reward: -95.854, mean reward: -5.325 [-100.100,  5.931], mean action: 7.389 [0.000, 15.000],  loss: 79.818500, mae: 47.017945, mean_q: -48.031241, mean_eps: 0.941185\n",
            "Track generation: 1091..1375 -> 284-tiles track\n",
            "  681/3000: episode: 38, duration: 5.171s, episode steps:  18, steps per second:   3, episode reward: -94.599, mean reward: -5.256 [-100.100,  6.767], mean action: 9.278 [0.000, 13.000],  loss: 59.375657, mae: 46.654515, mean_q: -47.980850, mean_eps: 0.939565\n",
            "Track generation: 1166..1469 -> 303-tiles track\n",
            "  699/3000: episode: 39, duration: 5.169s, episode steps:  18, steps per second:   3, episode reward: -95.266, mean reward: -5.293 [-100.100,  6.323], mean action: 9.056 [3.000, 15.000],  loss: 90.282895, mae: 46.579555, mean_q: -47.702453, mean_eps: 0.937945\n",
            "Track generation: 1241..1555 -> 314-tiles track\n",
            "  717/3000: episode: 40, duration: 5.423s, episode steps:  18, steps per second:   3, episode reward: -95.615, mean reward: -5.312 [-100.100,  6.090], mean action: 6.667 [0.000, 15.000],  loss: 44.370162, mae: 46.517292, mean_q: -47.561020, mean_eps: 0.936325\n",
            "Track generation: 961..1218 -> 257-tiles track\n",
            "  735/3000: episode: 41, duration: 5.130s, episode steps:  18, steps per second:   4, episode reward: -93.481, mean reward: -5.193 [-100.100,  7.513], mean action: 7.000 [1.000, 15.000],  loss: 92.888248, mae: 46.678994, mean_q: -47.535809, mean_eps: 0.934705\n",
            "Track generation: 1204..1509 -> 305-tiles track\n",
            "  753/3000: episode: 42, duration: 5.132s, episode steps:  18, steps per second:   4, episode reward: -95.332, mean reward: -5.296 [-100.100,  6.279], mean action: 6.444 [1.000, 14.000],  loss: 66.885255, mae: 47.368495, mean_q: -48.140937, mean_eps: 0.933085\n",
            "Track generation: 1139..1434 -> 295-tiles track\n",
            "  771/3000: episode: 43, duration: 5.121s, episode steps:  18, steps per second:   4, episode reward: -94.996, mean reward: -5.278 [-100.100,  6.503], mean action: 7.778 [0.000, 15.000],  loss: 67.671527, mae: 47.123682, mean_q: -48.033895, mean_eps: 0.931465\n",
            "Track generation: 1228..1546 -> 318-tiles track\n",
            "  789/3000: episode: 44, duration: 5.150s, episode steps:  18, steps per second:   3, episode reward: -92.682, mean reward: -5.149 [-100.200,  6.009], mean action: 5.333 [0.000, 14.000],  loss: 64.395554, mae: 46.798969, mean_q: -48.157283, mean_eps: 0.929845\n",
            "Track generation: 1136..1424 -> 288-tiles track\n",
            "  807/3000: episode: 45, duration: 5.456s, episode steps:  18, steps per second:   3, episode reward: -94.747, mean reward: -5.264 [-100.100,  6.669], mean action: 7.944 [0.000, 15.000],  loss: 63.289346, mae: 46.740487, mean_q: -48.038019, mean_eps: 0.928225\n",
            "Track generation: 1115..1398 -> 283-tiles track\n",
            "  825/3000: episode: 46, duration: 5.144s, episode steps:  18, steps per second:   3, episode reward: -94.562, mean reward: -5.253 [-100.100,  6.792], mean action: 6.389 [0.000, 15.000],  loss: 77.796995, mae: 46.595241, mean_q: -48.008325, mean_eps: 0.926605\n",
            "Track generation: 943..1185 -> 242-tiles track\n",
            "retry to generate track (normal if there are not manyinstances of this message)\n",
            "Track generation: 1284..1609 -> 325-tiles track\n",
            "  843/3000: episode: 47, duration: 5.150s, episode steps:  18, steps per second:   3, episode reward: -95.941, mean reward: -5.330 [-100.100,  5.873], mean action: 7.500 [0.000, 15.000],  loss: 60.766947, mae: 47.172534, mean_q: -48.594939, mean_eps: 0.924985\n",
            "Track generation: 1131..1418 -> 287-tiles track\n",
            "  861/3000: episode: 48, duration: 5.126s, episode steps:  18, steps per second:   4, episode reward: -91.314, mean reward: -5.073 [-100.200,  6.693], mean action: 6.500 [0.000, 14.000],  loss: 78.391936, mae: 47.243477, mean_q: -48.333677, mean_eps: 0.923365\n",
            "Track generation: 1111..1393 -> 282-tiles track\n",
            "  878/3000: episode: 49, duration: 4.857s, episode steps:  17, steps per second:   3, episode reward: -97.983, mean reward: -5.764 [-100.300,  6.817], mean action: 8.412 [0.000, 15.000],  loss: 72.803531, mae: 47.450907, mean_q: -48.880433, mean_eps: 0.921790\n",
            "Track generation: 1080..1354 -> 274-tiles track\n",
            "  896/3000: episode: 50, duration: 4.995s, episode steps:  18, steps per second:   4, episode reward: -94.211, mean reward: -5.234 [-100.100,  7.026], mean action: 5.667 [0.000, 14.000],  loss: 66.434760, mae: 47.295906, mean_q: -49.103329, mean_eps: 0.920215\n",
            "Track generation: 1109..1394 -> 285-tiles track\n",
            "retry to generate track (normal if there are not manyinstances of this message)\n",
            "Track generation: 1065..1335 -> 270-tiles track\n",
            "  914/3000: episode: 51, duration: 5.309s, episode steps:  18, steps per second:   3, episode reward: -94.048, mean reward: -5.225 [-100.100,  7.135], mean action: 7.500 [0.000, 15.000],  loss: 75.419963, mae: 47.215143, mean_q: -48.594720, mean_eps: 0.918595\n",
            "Track generation: 970..1220 -> 250-tiles track\n",
            "retry to generate track (normal if there are not manyinstances of this message)\n",
            "Track generation: 1246..1562 -> 316-tiles track\n",
            "  932/3000: episode: 52, duration: 5.043s, episode steps:  18, steps per second:   4, episode reward: -89.527, mean reward: -4.974 [-97.125,  6.049], mean action: 5.667 [0.000, 13.000],  loss: 68.311740, mae: 47.873851, mean_q: -48.796129, mean_eps: 0.916975\n",
            "Track generation: 1253..1570 -> 317-tiles track\n",
            "  950/3000: episode: 53, duration: 5.054s, episode steps:  18, steps per second:   4, episode reward: -95.706, mean reward: -5.317 [-100.100,  6.029], mean action: 6.000 [0.000, 15.000],  loss: 77.671548, mae: 48.238561, mean_q: -49.129746, mean_eps: 0.915355\n",
            "Track generation: 1083..1357 -> 274-tiles track\n",
            "  968/3000: episode: 54, duration: 5.010s, episode steps:  18, steps per second:   4, episode reward: -94.211, mean reward: -5.234 [-100.100,  7.026], mean action: 8.611 [2.000, 15.000],  loss: 59.342416, mae: 48.047050, mean_q: -48.699372, mean_eps: 0.913735\n",
            "Track generation: 1206..1512 -> 306-tiles track\n",
            "  986/3000: episode: 55, duration: 5.051s, episode steps:  18, steps per second:   4, episode reward: -95.364, mean reward: -5.298 [-100.100,  6.257], mean action: 8.722 [0.000, 15.000],  loss: 67.339648, mae: 47.485611, mean_q: -47.980985, mean_eps: 0.912115\n",
            "Track generation: 1116..1399 -> 283-tiles track\n",
            " 1003/3000: episode: 56, duration: 5.051s, episode steps:  17, steps per second:   3, episode reward: -98.008, mean reward: -5.765 [-100.300,  6.792], mean action: 9.000 [0.000, 13.000],  loss: 83.405432, mae: 47.916382, mean_q: -48.688382, mean_eps: 0.910540\n",
            "Track generation: 1280..1604 -> 324-tiles track\n",
            " 1021/3000: episode: 57, duration: 5.043s, episode steps:  18, steps per second:   4, episode reward: -95.912, mean reward: -5.328 [-100.100,  5.892], mean action: 7.167 [0.000, 15.000],  loss: 63.142339, mae: 48.584500, mean_q: -49.491027, mean_eps: 0.908965\n",
            "Track generation: 1150..1442 -> 292-tiles track\n",
            " 1038/3000: episode: 58, duration: 4.760s, episode steps:  17, steps per second:   4, episode reward: -98.227, mean reward: -5.778 [-100.300,  6.573], mean action: 8.471 [0.000, 15.000],  loss: 73.411534, mae: 48.699163, mean_q: -49.223774, mean_eps: 0.907390\n",
            "Track generation: 1048..1314 -> 266-tiles track\n",
            " 1056/3000: episode: 59, duration: 5.028s, episode steps:  18, steps per second:   4, episode reward: -93.879, mean reward: -5.216 [-100.100,  7.247], mean action: 7.778 [0.000, 15.000],  loss: 59.935279, mae: 48.500439, mean_q: -49.632238, mean_eps: 0.905815\n",
            "Track generation: 1160..1455 -> 295-tiles track\n",
            " 1074/3000: episode: 60, duration: 5.030s, episode steps:  18, steps per second:   4, episode reward: -94.996, mean reward: -5.278 [-100.100,  6.503], mean action: 7.889 [1.000, 14.000],  loss: 60.798568, mae: 47.973405, mean_q: -49.144730, mean_eps: 0.904195\n",
            "Track generation: 1287..1613 -> 326-tiles track\n",
            " 1091/3000: episode: 61, duration: 4.780s, episode steps:  17, steps per second:   4, episode reward: -98.946, mean reward: -5.820 [-100.300,  5.854], mean action: 9.647 [0.000, 15.000],  loss: 78.262869, mae: 48.199593, mean_q: -49.452726, mean_eps: 0.902620\n",
            "Track generation: 1236..1549 -> 313-tiles track\n",
            " 1109/3000: episode: 62, duration: 5.321s, episode steps:  18, steps per second:   3, episode reward: -92.479, mean reward: -5.138 [-100.200,  6.110], mean action: 6.944 [1.000, 14.000],  loss: 86.073452, mae: 48.873368, mean_q: -49.954947, mean_eps: 0.901045\n",
            "Track generation: 1112..1394 -> 282-tiles track\n",
            " 1126/3000: episode: 63, duration: 4.743s, episode steps:  17, steps per second:   4, episode reward: -97.983, mean reward: -5.764 [-100.300,  6.817], mean action: 8.824 [0.000, 15.000],  loss: 75.338932, mae: 49.179767, mean_q: -49.983604, mean_eps: 0.899470\n",
            "Track generation: 1007..1263 -> 256-tiles track\n",
            " 1143/3000: episode: 64, duration: 4.748s, episode steps:  17, steps per second:   4, episode reward: -97.257, mean reward: -5.721 [-100.300,  7.543], mean action: 9.529 [5.000, 15.000],  loss: 76.616064, mae: 49.266568, mean_q: -49.535027, mean_eps: 0.897940\n",
            "Track generation: 1239..1553 -> 314-tiles track\n",
            " 1161/3000: episode: 65, duration: 5.020s, episode steps:  18, steps per second:   4, episode reward: -95.615, mean reward: -5.312 [-100.100,  6.090], mean action: 7.833 [0.000, 15.000],  loss: 65.072893, mae: 49.150761, mean_q: -49.745493, mean_eps: 0.896365\n",
            "Track generation: 1171..1467 -> 296-tiles track\n",
            " 1179/3000: episode: 66, duration: 5.035s, episode steps:  18, steps per second:   4, episode reward: -95.031, mean reward: -5.279 [-100.100,  6.480], mean action: 6.778 [0.000, 15.000],  loss: 76.231397, mae: 48.743750, mean_q: -49.555166, mean_eps: 0.894745\n",
            "Track generation: 1115..1398 -> 283-tiles track\n",
            " 1197/3000: episode: 67, duration: 5.003s, episode steps:  18, steps per second:   4, episode reward: -94.562, mean reward: -5.253 [-100.100,  6.792], mean action: 6.167 [0.000, 14.000],  loss: 53.757544, mae: 48.883746, mean_q: -49.438668, mean_eps: 0.893125\n",
            "Track generation: 1101..1387 -> 286-tiles track\n",
            " 1215/3000: episode: 68, duration: 5.306s, episode steps:  18, steps per second:   3, episode reward: -94.674, mean reward: -5.260 [-100.100,  6.718], mean action: 8.056 [0.000, 15.000],  loss: 56.999777, mae: 48.504416, mean_q: -49.323612, mean_eps: 0.891505\n",
            "Track generation: 1098..1377 -> 279-tiles track\n",
            " 1233/3000: episode: 69, duration: 5.014s, episode steps:  18, steps per second:   4, episode reward: -90.912, mean reward: -5.051 [-100.200,  6.894], mean action: 5.833 [0.000, 15.000],  loss: 63.282232, mae: 48.337272, mean_q: -49.178521, mean_eps: 0.889885\n",
            "Track generation: 1126..1419 -> 293-tiles track\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}